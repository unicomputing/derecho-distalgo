## This is a DistAlgo implementation of Derecho, as described in
## Derecho: Fast State Machine Replication for Cloud Services, by
## SAGAR JHA, Cornell University, USA
## JONATHAN BEHRENS, Cornell University, USA and MIT, USA
## THEO GKOUNTOUVAS, MATTHEW MILANO, WEIJIA SONG, EDWARD TREMEL,
## ROBBERT VAN RENESSE, SYDNEY ZINK, and KENNETH P. BIRMAN,
## Cornell University, USA
## ACM Transactions on Computer Systems, Vol. 36, No. 2. Article 4. March 2019.
## http://www.cs.cornell.edu/ken/derecho.pdf
##
## Implementation of the View Change Protocol as listed in 
## Appendix A: Pseudo-code for key protocol steps.  Pages 32 to 37.

import sys
import random
import time
import os

class Slot:
  """a slot that store a message"""

  # A.2.1 SST Structure. SMC uses two fields, slots and received_num. 
  # slots is a vector of window_size slots, 
  # each of which can store a message of up to max_message_size characters. 
  # The index associated with a slot is used to signal that 
  # a new message is present in that slot: 
  # For example, if a slot's index had value k and transitions to k + 1, 
  # then a new message is available to be received.
  # The vector received_num holds 
  # counters of the number of messages received from each node.
  def __init__(self):
    self.index = 0   ## index position of this slot in vector of slots
    self.buf = None  ## message in this slot, of up to max_message_size characters
    self.size = 0    ## size of message in this slot, up to max_message_size

class SSTRow:  ## The variables and their initial values are not explicitly mentioned in the paper
  """a row in the SST: shared state table"""

  # Sec 3.4 Shared State Table: The SST. 2nd paragraph, Lines 1-6:
  # Derecho uses protocols that run on a novel replicated data structure 
  # called the shared state table, or SST. 
  # The SST offers a tabular distributed shared memory abstraction. 
  # Every member of the top-level group holds 
  # its own replica of the entire table, in local memory.
  # Within this table, there is one identically formatted row per member. 
  # A member has full read/write access to its own row but
  # is limited to read-only copies of the rows associated with other members.
  # Page 12, Sec 3.4 The SST offers a tabular distributed shared memory abstraction. 
  # Every member of the top-level group holds its own replica of the entire table, in local memory. (source: 3.4 Shared State Table: The SST, paragraph 2, line 4)
  # Within this table, there is one identically formatted row per member. A member has full read/write access to its own row 
  # but is limited to read-only copies of the rows associated with other members
  ## In this implementation a list of SSTRows objects form a Shared State Table (SST)
  ## A copy of the SST is stored by each Node.
  #
  # A.1.1 SST.
  # column_name->string|string[int] // e.g. wedged or latest_received_index[3]
  # sst_row->sst[row_rank]
  # row_rank->int
  # sst_column->sst[*].column_name
  # sst_entry->sst_row.column_name // e.g. sst[0].stable_msg_index[0]
  def __init__(self, num_nodes, window_size):
    ## each field is a column name.
    ## if field is a LIST, it stores a value for each other node based on the index of the node.
    self.suspected = [False] * num_nodes ## LIST; for each node, whether the node is suspected to be crashed
    self.num_committed = 0     ## number of changes committed by the current node, as suggested by the Leader
    self.num_acked = 0         ## number of changes acknowleged by the current node, as suggested by the Leader
    self.received_num = [-1] * num_nodes ## LIST; for each node, number of messages received by the current node
    self.wedged = False        ## whether the current node has wedged its SST
    self.changes = []          ## list of node ids of crashed/new nodes
    self.num_changes = 0       ## number of changes, i.e., length of self.changes
    self.num_installed = 0     ## number of `changes` applied or installed to the current node's SST
    self.min_latest_received = [-1] * num_nodes    ## LIST; for each node, minimum index of message received from the node
    self.latest_received_index = [-1] * num_nodes  ## LIST; for each node, index of latest message received from the node
    self.ragged_edge_computed = True     ## whether the ragged trim is finalized for this node after a failure 
    self.latest_delivered_index = -1     ## index of the last message successfully delivered by the current node to all nodes
    self.global_index = -1               ## global message index counter, used for total ordering of messages in the system
    self.slots = [Slot() for x in range(window_size)]  ## each Slot stores the new message recieved by the current node

class View(): 
  """ ## A view holds the metadata of an epoch""" ## Understood from Fig.11 page 17
  def __init__(self, num_nodes, epoch=0, leader_rank=0):
    self.leader_rank = leader_rank        ## rank of the leader in the current view
    self.failed = [False] * num_nodes     ## true against index of each node's rank if a process is deemed 'failed' in the view
                                          ## (for each node, whether the node is deemed 'failed' in the current view)
    self.wedged = False                   ## whether the view is 'wedged' by the system
    self.members = [None] * num_nodes     ## list of members (nodes) in the current view
    self.epoch = epoch                    ## Current epoch of the system

  def add_member(self, node_id):          ## Add member to the current view
    self.members.append(node_id)          ## append the member/node to the list of members
    self.failed.append(False)             ## add the failed attribute corresponding to the removed node

  def remove_member(self, node_id):       ## Remove member to the current view
    try:
      index = self.members.index(node_id) ## get the index of the member/node to be deleted
      self.members.remove(node_id)        ## remove the member/node to the list of members
      del self.failed[index]              ## remove the failed attribute corresponding to the removed node
    except Exception as e:
      output("Exception occured while removing member: ", node_id, " from the view. ", e)
      return

class Node(process):

  ## Simulate RDMA based write to an SST
  ## Every Node owns a row in the SST and is the only writer. 
  ## Every local update to a cell must be multicasted to all other Nodes to update their copy of the same row.
  ## Derecho uses RDMA to update across Nodes, we simulate this using the below functions write_sst and write_view.
  ## Every update to a cell is written locally and then multicasted to all Nodes for them to update their local copies.
  ## row: row number in the SST.
  ## col: column name to be updated.
  ## val: value with which the column has to be updated.
  ## index: (optional) if the column is a list, pass the index in the list to be updated
  def write_sst(row, col, val, index=None):  
    update_sst(row, col, val, index)         ## update the local SST
    message = ('rdma_write_sst', row, col, val, index, curr_view.epoch)  ## create an update message to broadcast the update to other nodes
    message_tagged = ('data' if message[2] == "slots" else 'control',) + message ## new for separating data and control
    output("====", message_tagged)
    send(message_tagged, to=others)  ## send the update message to other nodes, so they update their local copies

  ## The following aggregation functions are not defined in the paper.
  def min_not_failed(col):
    """min value of column named col for non-suspected nodes in the sst of this node"""
    return min({getattr(sst[row], col) for row in range(len(sst)) if not sst[my_rank].suspected[row]})

  def min_(col, index = None):
    """min value of column col, at given index for col being a list, in the sst of this node"""
    try:
      if index is None:
        return min({getattr(sst[row], col) for row in range(len(sst))})
      else:
        return min({getattr(sst[row], col)[index] for row in range(len(sst))})
    except:
      return -1

  def logical_and_not_failed(col):
    """logical 'AND' of all values in column col for non-suspected nodes in the sst of this node"""
    return all(getattr(sst[row], col) for row in range(len(sst)) if not sst[my_rank].suspected[row])

  def logical_or(col):
    """logical 'OR' of all values in column col in the sst of this node"""
    return any(getattr(sst[row], col) for row in range(len(sst)))

  def min_with_val(col, val):
    """minimum rank of row in the sst of this node where column col has value val"""
    return min({row for row in range(len(sst)) if getattr(sst[row], col) == val})

  def Count(col, val):
    """count of rows with value val for column col in the sst of this node"""
    return len({row for row in range(len(sst)) if getattr(sst[row], col) == val})

  def get_max_gi():
    """max value of the global index of all the messages in the system,
       max of (sst[my_rank].min_latest_received[n] âˆ— |G| + n) over n in 1..|G|"""
    if len(G) < 1: return 0
    return max((sst[my_rank].min_latest_received[n] * len(G) + n) for n in range(len(G)))

  # A.1.2 Message Ordering
  # Message: M(i, k) represents a message with i as the sender rank and 
  # k as the sender index.
  # For example, the zeroth message by sender number 2 is M(2, 0). 
  # We have a round-robin ordering imposed on messages. 
  # M(i1, k1) < M(i2, k2) <==> k1 < k2 ||(k1 == k2 and i1 < i2).
  # The global index of M(i, k), gi(M(i, k)) is the position of this message
  # in the round-robin ordering. 
  # So M(0, 0) has a global index of 0, M(1, 0) has a global index of 1, 
  # and so on
  def gi(i, k):
    """global index of a message given sender node's rank i and sender's index k of the message""" 
    return (k) * len(G) + i               # gi(M(i, k)) = i + |G| * k

  # A.2.2 Initialization.
  def setup(nodes, my_rank, window_size, sim):
                                          # for i in 1 to n {
                                          #   for j in 1 to n {
                                          #     sst[i].received_num[j] = -1; }
                                          #   for k in 1 to window_size {
                                          #     sst[i].slots[k].buf = nullptr
                                          #     sst[i].slots[k].index = 0 }}
    self.num_nodes = len(nodes)           ## number of nodes in the system
    self.sst = [SSTRow(num_nodes, window_size) for _ in range(num_nodes)]   ## initialized the local SST for this node
    self.sent_num = -1                    # sent_num = -1   ## helper variable to maintain a count of messages sent by this node
    self.curr_view = View(num_nodes)      ## initialize the local view object
    self.G = nodes                        ## the set of nodes in the system
    self.msgs = {}                        ## dictionary, messages sent/received by this node, indexed by the global index of each message
    self.max_msg_size = 10                ## limit on the maximum message size for the system
    for i, node in enumerate(nodes):
      self.curr_view.members[i] = node    ## initially updating the 'members' list of the view
    self.results = {}                     ## store the result/messages processed by the system
    self.others = [n for n in nodes if n != self] ## list of other nodes in the system
    output('initial nodes configuration: ', nodes)

  def run():
    while True:
      --receive_messages

      choice = random.choice(['recv', 'stable', 'suspect', 'elect', 'other'])
      if choice == 'recv':      receive_msg()         # always
      elif choice == 'stable':  stability_delivery()  # always
      elif choice == 'suspect': suspect()             # always
      elif choice == 'elect':   leader_selection()    # always

      # A.4.2 Terminating old view and installing new view after wedging.
      elif (sst[curr_view.leader_rank].num_changes > sst[my_rank].num_acked    # when (sst[leader_rank].num_changes > sst[my_rank].num_acked) {
            and curr_view.leader_rank != my_rank):                          
          output("leader proposed a new change")                               #   // |= leader proposed a new change
                                                                               #   if(curr_view.leader_rank != my_rank) {
          leader_rank = curr_view.leader_rank  
          write_sst(my_rank, 'num_acked', sst[leader_rank].num_changes)        ## missing but is needed to terminate; acknowledge the changes
          write_sst(my_rank, 'num_changes', sst[leader_rank].num_changes)      #     sst[my_rank].num_changes = sst[leader_rank].num_changes;
                                                                               #     // copy entire changes vector from the leader's row
          write_sst(my_rank, 'changes', sst[leader_rank].changes)              #     sst[my_rank].changes = sst[leader_rank].changes;
          write_sst(my_rank, 'num_committed', sst[leader_rank].num_committed)  #     sst[my_rank].num_committed = sst[leader_rank].num_committed;
          curr_view.wedged = True                                              #     curr_view.wedge();
          write_sst(my_rank, 'wedged', True)                                   #     sst[my_rank].wedged = true;
                                                                               #     // |= acknowledged leader's proposal and wedged the current view
                                                                               #   }}
      elif (curr_view.leader_rank == my_rank and                               # when (curr_view.leader_rank == my_rank and 
            min_not_failed('num_acked') > sst[my_rank].num_committed):         #       MinNotFailed(sst[âˆ—].num_acked) > sst[my_rank].num_committed) {
          output("commit_proposal_leader")                                     #   // |= K U\F ( acknowledged a new proposal )
          write_sst(my_rank, 'num_committed', min_not_failed('num_acked'))     #   sst[my_rank].num_committed = MinNotFailed(sst[âˆ—].num_acked);
                                                                               #   // |= commited acknowledged proposals
                                                                               # }
      elif sst[curr_view.leader_rank].num_committed > sst[my_rank].num_installed:
                                                   # when (sst[my_rank].num_committed[leader_rank] > sst[my_rank].num_installed[my_rank]) {
                                                   #   // |= leader committed a new membership change
          output ('leader committed a new membership change')
          curr_view.wedged = True                  #   curr_view.wedge();
          write_sst(my_rank, 'wedged', True)       #   sst[my_rank].wedged = true;
          await(logical_and_not_failed('wedged'))  #   when (LogicalAndNotFailed(sst[âˆ—].wedged) == true) {  ## todo: check liveness
                                                   #     // |= K U\F (current view is wedged)
          terminate_epoch()                        #     terminate_epoch(); }}

  # A.2.3 Sending. First the sending node reserves one of the slots:
  ## resolve the available slot before sending the message
  def get_buffer(msg_size):                       # char* get_buffer(msg_size) { ## returns true if slot is available
    output("in get_buffer function, msg_size: ", msg_size)
    if msg_size > max_msg_size: return False      #   assert(msg_size <= max_msg_size);
                                                  #   // A Slot can be reused if the previos message in that slot was received by everyone
                                                  #   // Combine it with the FIFO ordering of messages
    completed_num = min_("received_num", my_rank) #   completed_num = Min{sst[*].received_num[my_rank]};
    if sent_num - completed_num >= window_size:   #   if (sent_num - completed_num < window_size) { ## ERRATA if (sent_num - completed_num > window_size) ### > modified to >= to avoid deadlock
                                                  ### cases like window_size = 10, sent_num = 10, completed_num = 0, here the data in slot[1] would get over-ridden, and thus system reaching a deadlock
      output("slot buffer seems to be full, returning, sent-num: ", sent_num, " completed-num: ", completed_num)
      return False                                #     return nullptr; } ## modified, returns False to signify failure 
    slot = (sent_num + 1) % window_size           #   slot = (sent_num + 1) % window_size;
    sst[my_rank].slots[slot].size = msg_size      #   sst[my_rank].slots[slot].size = msg_size;
    return True                                   #   return sst[my_rank].slots[slot].buf; }  ## modified to return True iff successeful 
  
  ## increment the slot and send the message to other nodes
  def send_msg(msg):
    output("in send_msg function, msg: ", msg)  
    if is_failure_detected():                     ## the system ceases to process any new request if there is a failure of a node in the system
      output("Failure detected, new message will not be sent")
      return 
    is_buffer_available = get_buffer(0 if msg is None else len(msg)) ## check if slot buffer is available
    if not is_buffer_available:                   #   // After get_,buffer returns a non-null buffer, the application 
      return                                      #   // writes the message contents in the buffer and calls send
    slot = (sent_num + 1) % window_size           #   slot = (sent_num + 1) % window_size;
    sst[my_rank].slots[slot].buf = msg            #   the application writes the message contents in the buffer
    sst[my_rank].slots[slot].index += 1           #   sst[my_rank].slots[slot].index++;
    output("msg for (i,k): (",my_rank, ",", sent_num+1,")")
    write_sst(my_rank, "slots", sst[my_rank].slots[slot], slot)     
    sent_num += 1                                 #   sent_num++; }
  
  def receive(msg=('request', msg),from_=p):      ## receive request from application (sim)
    if msg[0] in results:
      output("Duplicate request received. Msg: ", msg, ". Sending result: ", results[msg[0]], " back.")
      send(('response', results[msg[0]]), to= p)  ## send the corresponding response back
      return
    send_msg(msg)                         ## send the message in the next slot

  # A.2.4 Receiving.
  ## receive a message and increment the current slot based on the window size
  def receive_msg():                                       # always {
    # output("in receive_msg()")
    for i in range(num_nodes):                             #   for i in 1 to n {
                                                           #    // the next message from node i will arrive in this slot
      slot = (sst[my_rank].received_num[i]+1)%window_size  #    slot = (sst[my_rank].received_num[i]+1)%window_size
      if sst[i].slots[slot].index == (sst[my_rank].received_num[i]+1)//window_size+1:
                                                           #    if(sst[i].slots[slot].index == (sst[my_rank].received_num[i]+1)/window_size+1) { 
        output('new message available for node: ', nodes[i], 'in slot:', slot, ', at index: ', sst[i].slots[slot].index, ' and received_num: ', (sst[my_rank].received_num[i]+1))
        write_sst(my_rank, "received_num", sst[my_rank].received_num[i]+1, i)
                                                           #      ++sst[my_rank].received_num[i];
        recv((i, sst[my_rank].received_num[i]))            #      recv(M(i, sst[my_rank].received_num[i])); }}}

  ## minimun index received by this node and the lagging node according to this Node's SST
  def get_min_idx_rec():  # (min,argmin)i sst[my_rank].latest_received_index[i];   
    min_ind = min(sst[my_rank].latest_received_index[rank] for rank in range(len(sst[my_rank].latest_received_index)))### removed the if condition, not sure why would this be required
    lagging_node = min(rank for rank in range(len(sst[my_rank].latest_received_index)) if sst[my_rank].latest_received_index[rank] == min_ind)  ## node from minimum latest index received is the lagging node as per this node's reference
    ### issue, imagine, n = 5, currently message has (0,0) and (1,1) but this function used to return last node as lagging, and thus, may cause some message to be left behind is (2,0) is missing and (3,0) is present, which then gets delivered
    return min_ind,lagging_node

  # A.3 Atomic Multicast Delivery in the Steady State
  # A.3.1 Receive.
  ## add received message M to the global message queue
  def recv(M):                                         # on recv(M(i,k)) {
    output("in recv function, M: ", M)                 #   // |= K_me(Received M(i,k))
                                                       #   // store the message to deliver later
    (i, k) = M
    ### the following is added to avoid stalls, it sends a null message for the current sequence number
    if my_rank < i and k > sent_num:      ##  if(my_rank < i && I have not sent M(my_rank, k)) { ### changed, sent_num should be used to check the message number 
      for x in range(k-sent_num):                       ## for every skipped slot ### Resolves Issue: when the buffer is full, the null message for this global number is skipped
        send_msg(None)                                  ##  send a null message 
    elif my_rank > i and k > sent_num+1:  ## else if(my_rank > i && I have not sent M(my_rank, k-1)) {
      for x in range(k-sent_num-1):                     ## for every skipped slot ### Resolves Issue: when the buffer is full, the null message for this global number is skipped
        send_msg(None)                                  ## send a null message

    msgs[gi(i, k)] = (i, k)                            #   msgs[hi(M(i,k))] = M(i,k);
    write_sst(my_rank, "latest_received_index", k, i)  #   sst[my_rank].latest_received_index[i] = k;    
                                                       #   // calculate global index of the message in the global roundâˆ’robin ordering
    min_index_received, lagging_node_rank = get_min_idx_rec()
                                                       #   (min_index_received, lagging_node_rank) =
                                                       #       (min,argmin) i sst[my_rank].latest_received_index[i];  
    write_sst(my_rank, "global_index", (min_index_received + 1) * len(G) + lagging_node_rank - 1)
                                                       #   sst[my_rank].global_index = (min_index_received + 1) âˆ— |G| + lagging_node_rank âˆ’ 1;
                                                       #   // |= K_me (Received all messages M(i,k) s.t. Ð´i(M(i,k)) â‰¤ sst[my_rank].global_index)
                                                       # }

  def deliver_upcall(index, msg):
    # output("in deliver_upcall function; index: ", index, " msg: ", msg)
    if sst[msg[0]].slots[msg[1]%window_size].buf is not None:
      results[sst[msg[0]].slots[msg[1]%window_size].buf[0]] = sst[msg[0]].slots[msg[1]%window_size].buf[0] ## No implementation provided in the pseudocode
      send(('response', sst[msg[0]].slots[msg[1]%window_size].buf[0]), to= sim)   ## Send the response back to sim
      output("response sent to the client/sim, index: ", index, "and response is ", sst[msg[0]].slots[msg[1]%window_size].buf[0])

  # A.3.2 Stability and Delivery
  ## Calculates the next global index for message delivery 
  def stability_delivery():                   # always {
    # output("in stability_delivery()")
    stable_msg_index = min_("global_index")   #   stable_msg_index = Min{sst[âˆ—].global_index}
                                              #   // |= K_me (âˆ€p âˆˆ G : K p (Received all messages M(i,k) s.t. Ð´i(M(i,k)) â‰¤ sst[my_rank].global_index))
    to_delete = []
    for msg_index in sorted(msgs):                 #   for (msg : msgs) { ### msgs must be delivered in the global order ### Sorting the dictionary to process messages in a monotically increasing sequence
      if msg_index <= stable_msg_index:            #     if (msg.global_index <= min_stable_msg_index) {
        deliver_upcall(msg_index, msgs[msg_index]) #       deliver_upcall(msg);
        to_delete.append(msg_index)                ##  store the messages that needs to be deleted, to delete later 
    for d in to_delete:
      del msgs[d]                             #   msgs.remove(msg.global_index); }} ###  msg is a tuple, here d is a tuple
    if to_delete:                             ## if there are new changes, only then update latest_delivered_index. Since this function gets called always
                                              ## it reduces the number of control messages sent in the system
      write_sst(my_rank, "latest_delivered_index", stable_msg_index)
    # write_sst(my_rank, "latest_delivered_index", stable_msg_index)
                                              #   sst[my_rank].latest_delivered_index = stable_msg_index }
                                              # // |= K_me (Delivered all messages â‰¤ sst[my_rank].latest_delivered_index)
  
  # A.4 View Change Protocol
  # A.4.1 Failure Handling and Leader Proposing Changes for Next View.
  ## Receives a failure trigger.
  ## This simulates polling on SST for not receving a message for a time-period from one of the nodes
  def receive(msg=('failure', r)):              # every 1 millisecond {
                                                #   post RDMA write with completion time to every SST Row that is not frozen
 						                                    #   if (no completion polled from row r) {
      output("freeze", r)                       #     sst.freeze(r); ### ?? # no impl mentioned? Can we have a field checking if the sst is frozen or not         
      report_failure(r)                         #     report_failure(r); }}

  ## update the suspected field upon noticing a node failure
  def report_failure(r):                              # report_failure(r) {
    output("in report_failure function")              #   // local node suspects node that owns the row r
    write_sst(my_rank, 'suspected', True, r)          #   sst[my_rank].suspected[r] = true;
    total_failed = Count('suspected', True)           #   total_failed = Count(sst[âˆ—].suspected, true);
    if total_failed >= (num_nodes + 1)/2:             #   if (total_failed >= (num_members + 1)/2) {
      output("ERROR: derecho_partitioning_exception") #     throw derecho_partitioning_exception;
      return                                          #   }
                                                      # }

  def find_new_leader(rank):                # find_new__leader(r) {
                                            #   // returns the node that r believes to be the leader, on the basis of the current sst.
                                            #   // Notice that becasue of asynchrony in the sst update propagations, 
                                            #   // callers other than r itself might be using old version's of r's suspicion set, 
                                            #   // in which cse the caller could obtain an old leader belif of r's.
                                            #   // This is safe because leader beliefs are monotonic (they are ascending, in rank order).
    for i in range(len(curr_view.members)): #   for (int i = 0; i < curr_view.max_rank; ++i) {
      if sst[rank].suspected[i]: continue   #     if(sst[r].suspected[i]) continue;
      else: return i                        #     else return i }}

  ## update the view at the end of the current leader
  def leader_selection():                                              # always {
                                                                       # // Wait until all non-suspected nodes comsider me to be the leader.
                                                                       # // This implies that the new leader takes action only after
                                                                       # // every healthy node has pushed final nReceived data to it.
    # output("in leader_selection()")
    new_leader = find_new_leader(my_rank)                              # new_leader = find_new_leader(my_rank)
    if new_leader != curr_view.leader_rank and new_leader == my_rank:  # if(new_leader != curr_view.leader_rank && new_leader == my_rank)
      output("is the previous leader suspected?")
      # all_others_agree = True                                        #   bool all_others_agree = true
                                                                       #   // so as long as I continue to believe I will be leader
      while find_new_leader(my_rank) == my_rank:                       #   while(find_new_leader(my_rank) == my_rank) {
                                                                       #     // check if everyone else agrees I am leader
        --receive_messages                                     ## receive pending messages, necessary to receive updates to sst and thus updates to find_new_leader() 
        all_others_agree = True                                        ## bool all_others_agree = true ### moved the assignment inside the loop, if this ever becomes false, then this never becomes true and system is stuck
        for r in range(len(sst)):                                      #     for(r: SST.rows) {
          if not sst[my_rank].suspected[r]:                            #       if(sst[my_row].suspected[r] == false)
            all_others_agree = all_others_agree and (find_new_leader(r) == my_rank)
                                                                       #         all_others_agree &&= (find_new_leader(r) == my_rank) }

        if all_others_agree:                                           #     if(all_others_agree) {
                                                                       #     // Scan sst to learn prior proposals
          self.curr_view.leader_rank = my_rank                         #       curr_view.leader_rank = my_rank;
          # write_view('leader_rank', my_rank)                         ### write_view call is removed, as this can independently be computed by the nodes
          break                                                        #       break; }}}
    else:                                                              ## if new_leader != my_rank
      self.curr_view.leader_rank = find_new_leader(my_rank)            ##   set the new leader as the current view's leader

  ## poll the suspected field in the SST and propagate the status across the systems, page 36
  def suspect():                                         # always {
    # output("in suspect() function")
    for r in range(len(sst)):                            #   for(every row r and s) {
      for s in range(len(sst)):
        if sst[r].suspected[s] and not sst[my_rank].suspected[s]: #     if(sst[r].suspected[s] == true) { ### added a condition to check if sst[my_rank].suspected[s] is already true
                                                         #       // failure propagation---local node also suspects s
          write_sst(my_rank, 'suspected', True, s)       #       sst[my_rank].suspected[s] = true
                                                         #     }}
    for s in range(len(sst)):                            #   for(s=0; s < num_members; ++s) {
                                                         #     // if s is newly suspected
      if sst[my_rank].suspected[s]:  ### and (not curr_view.failed[s])  ### removed the 2nd conjunct to keep live when leader is killed
                                                         #     if sst[my_rank].suspected[s] == true and curr_view.failed[s] == false {
                                                         #       freeze(s)  ### ? No implementation found?
        report_failure(s)                                #       report_failure(s)
                                                         #       // mark s as failed in the current view
        curr_view.failed[s] = True                       #       curr_view.failed[s] = true
                                                         #       // |= s in F
                                                         #       // removes predicates defined in section 1 so that no new message can be sent or delivered
        curr_view.wedged = True                          #       curr_view.wedge()
        ### ?? B.3 marks its SST row as wedged before pushing entire SST row to all non-failed
        ### top-level group members. Missing 'non-failed' check?
        write_sst(my_rank, 'wedged', True)               #       sst[my_rank].wedged = true
        # leader_selection()                             ## adding a call for leader selection, to maintain sequence of actions ### removed, as this is said to be always

        if curr_view.leader_rank == my_rank and nodes[s] not in sst[my_rank].changes:
                                                         #       if(curr_view.leader_rank == my_rank and sst[my_rank].changes.contains(s) == false) { ### changes contains node id's not indices
          node_id = nodes[s]
          output("changes list updated with: ", node_id)
          # next_change_index = sst[my_rank].num_changes âˆ’ sst[my_rank].num_installed; ## not required in python for position
          changes = sst[my_rank].changes                               
          changes.append(node_id) 
          write_sst(my_rank, 'changes', changes)         #         sst[my_rank].changes[next_change_index] = id of node owning s
          num_changes = sst[my_rank].num_changes + 1     #
          write_sst(my_rank, 'num_changes', num_changes) #         sst[my_rank].num_changes++;
          write_sst(my_rank, 'num_acked', num_changes)   ##        update the leader's (self) num_acked field to num_changes (for the membership execution condition to be true for leader)
                                                         #         // |= proposed a new membership change and wedged the current view
                                                         #       }}}}
                                                                  
  # continuing A.4.2 Terminating old view and installing new view after wedging.
  def terminate_epoch():                                 # terminate_epoch() {
                                                         #   //calculate next view membership
    output("in terminate_epoch()")
    --receive_messages                                ## yeild to deliver all pending messages
    leader_rank = curr_view.leader_rank
    committed_count = sst[leader_rank].num_committed - (sst[leader_rank].num_installed)
                                                         #   committed_count = sst[leader_rank].num_committed âˆ’ sst[leader_rank].num_installed;
    next_view = View(num_nodes, curr_view.epoch+1)       ## creating a new view object
    next_view.members = curr_view.members                #   next_view.members = curr_view.members
    for change_index in range(committed_count):          #   for (change_index = 0; change_index < committed_count; change_index++) {
      node_id = sst[my_rank].changes[change_index]       #      node_id = sst[my_rank].changes[change_index]; 
      if node_id == self:                                ## if self is part of the change list, shutdown the node
        output("I am being removed from the system, shutting myself down now")
        exit()
                                                         #     // if node already a member, the change is to remove the node;
      if node_id in next_view.members:                   #     if (curr_view.contains(node_id) == true) {
        output("Removing node/member to the system: ", node_id)
        next_view.remove_member(node_id)                 #       new_view.members.remove(node_id); } ## new_view should be next_view
      else:                                              #     else {
        output("Adding new node/member to the system: ", node_id)
        next_view.add_member(node_id)                    #       next_view.members.append(node_id); }} 
    if leader_rank == my_rank:                           #   if (leader_rank == my_rank) {
      leader_ragged_edge_cleanup()                       #     leader_ragged_edge_cleanup();
    elif await(sst[leader_rank].ragged_edge_computed):   #   else { when (sst[leader_rank].ragged_edge_computed == true) { ## todo: check liveness
      non_leader_ragged_edge_cleanup()                   #     non_leader_ragged_edge_cleanup(); }}
    curr_view = next_view                                #   curr_view = next_view;
    reset_system(curr_view, sst[my_rank], committed_count) ## reset the system, reinitialize the sst and other metadata
    output ('new view has been installed with members: ', curr_view.members)#   // |= new view installed                                                         
                                                         # }                  

  ## Cleans up and corrects the ragged edge due to failure, for the leader node
  def leader_ragged_edge_cleanup():                      # leader_ragged_edge_cleanup() {
    # output("leader_ragged_edge_cleanup ", self)
    if logical_or("ragged_edge_computed"):               #   if (LogicalOr(sst[âˆ—].ragged_edge_computed) == true) {
      rank = min_with_val("ragged_edge_computed", True)  #     Let rank be s.t. sst[rank].ragged_edge_computed is true
                                                         #     // copy min_latest_received from the node that computed the ragged edge
      for n in range(len(G)):                            #     for (n = 0; n < |G|; ++n) {
        write_sst(my_rank, "min_latest_received", sst[rank].min_latest_received[n], n)
                                                         #       sst[my_rank].min_latest_received[n] = sst[rank].min_latest_received[n]; }
      write_sst(my_rank, "ragged_edge_computed", True)   #     sst[my_rank].ragged_edge_computed = true; }
    else:                                                #   else {
      for n in range(len(G)):                            #     for (n = 0; n < |G|; ++n) {
        write_sst(my_rank, "min_latest_received", min_("latest_received_index", n), n)
                                                         #       sst[my_rank].min_latest_received[n] = Min(sst[âˆ—].latest_received_index[n]);
                                                         #       // |= K_me (sst[my_rank].min_latest_received[n] number of messages from n are safe for delivery) }
      write_sst(my_rank, "ragged_edge_computed", True)   #     sst[my_rank].ragged_edge_computed = true;
    deliver_in_order()                                   #   deliver_in_order(); }
                                              
  ## Cleans up and corrects the ragged edge due to failure, for non-leader nodes
  def non_leader_ragged_edge_cleanup():              # non_leader_ragged_edge_cleanup() {
    # output("in non_leader_ragged_edge_cleanup function")
    leader_rank = curr_view.leader_rank              #   // copy from the leader
    for n in range(len(G)):                          #   for (n = 0; n < |G|; ++n) {
      write_sst(my_rank, "min_latest_received", sst[leader_rank].min_latest_received[n], n)
                                                     #     sst[my_rank].min_latest_received[n] = sst[leader_rank].min_latest_received[n]; }
    write_sst(my_rank, "ragged_edge_computed", True) #   sst[my_rank].ragged_edge_computed = true;
    deliver_in_order()                               #   deliver_in_order(); }

  ## Once the ragged edge has been cleaned up, the pending messages are delivered
  def deliver_in_order():                                    # deliver_in_order() {
    # output("in deliver_in_order function, messages list: ", msgs)
    curr_global_index = sst[my_rank].latest_delivered_index  #   curr_global_index = sst[my_rank].latest_delivered_index;
    max_global_index = get_max_gi()                          #   max_global_index = max over n of (sst[my_rank].min_latest_received[n] âˆ— |G| + n);
    for global_index in range(curr_global_index + 1, max_global_index + 1):
                                                             #   for (global_index = curr_global_index + 1; global_index <= max_global_index; ++global_index) {
      sender_index = global_index / len(G)                   #     sender_index = global_index / |G|;
      sender_rank = global_index % len(G)                    #     sender_rank = global_index % |G|;
      if (sender_index <= sst[my_rank].min_latest_received[sender_rank]):
                                                             #     if (sender_index <= sst[my_rank].min_latest_received[sender_rank]) {
        output("delivering the message with global index ", global_index, " and message ", msgs[global_index])
        deliver_upcall(global_index, msgs[global_index])     #       deliver_upcall(msgs[global_index]); }}}

  ## Recieves message for an RDMA write from any node and updates the local SST
  def receive(msg = (tag, 'rdma_write_sst', row, col, val, index, epoch)):  ## added _ to filter out 'data' and 'control'
    # output("Received message tag is: ", tag, " and msg: " , ('rdma_write_sst', row, col, val, index, epoch))
    if epoch != curr_view.epoch:
      output('rdma_write_sst message received for different epoch, current: ', curr_view.epoch, ' and message: ', epoch)
      return

    if is_failure_detected() and tag == 'data':
      output("Failure detected, no new data messages will be accepted")
      return

    update_sst(row, col, val, index)      ## update the local SST

  ## Reset the system after a membership change in the new epoch
  def reset_system(view, old_sst_row, num_changes_installed):
    self.nodes = view.members             ## nodes in the system based on the members of the given view
    self.num_nodes = len(nodes)           ## number of nodes in the system
    self.G = nodes                        ## nodes in the system, used to calculate global index
    self.sent_num = -1                    ## reset the sent_num in the new epoch
    self.msgs = {}                        ## reset the messages dictionary
    self.sst = [SSTRow(num_nodes, window_size) for _ in range(num_nodes)] ## reinitialize sst 
    self.my_rank = view.members.index(self) ## set my_rank based on the index in the new_view
    write_sst(my_rank, 'changes', old_sst_row.changes[num_changes_installed:]) ## copy over the pending changes to the new view's sst
    write_sst(my_rank, 'num_installed', old_sst_row.num_installed + num_changes_installed) ## update the number of changes installed with new changes installed
    write_sst(my_rank, 'num_changes', old_sst_row.num_changes)  ## copy over the number of changes to the new view's sst
    write_sst(my_rank, 'num_committed', old_sst_row.num_committed) ## copy over the number of committed changes to the new view's sst
    write_sst(my_rank, 'num_acked', old_sst_row.num_acked) ## copy over the number of acknowledged changes to the new view's sst

  def is_failure_detected():
    for r in range(len(sst)):                     ##  for(r: SST.rows) {
      if sst[my_rank].suspected[r]:               ##    if sst[my_rank].suspected[r]    
        return True                               ## // stop sending any new message
    return False

  def update_sst(row, col, val, index=None):  ## update the local SST
    if index is not None:                     ## check if a non null 'index' is passed, implies updating a list
      newval = getattr(sst[row], col)         ## retrieve the current list into newval
      newval[index] = val                     ## update the value at index to the val passed
      setattr(sst[row], col, newval)          ## update the local sst row with newval
    else:
      setattr(sst[row], col, val)             ## if it's not a list, just set the value to the new val passed

## The rest is for simulation and testing

class RequestMetrics:
  """object holding metrics for a request"""
  def __init__(self, cpu_start_time, run_start_time):
    self.success = False                  ## maintains the success status of the request
    self.cpu_start_time = cpu_start_time  ## cpu clock time at the time of sending request
    self.cpu_end_time = 0.0               ## cpu clock time at the time of receiving the response for the request 
    self.run_start_time = run_start_time  ## system time at the time of sending request
    self.run_end_time = 0.0               ## system time at the time of receiving the response for the request 

class Sim(process):
  """simulator for a failure scenario"""
  def setup(nodes, num_requests, test_failure, msg_size):
    self.metrics = {}                     ## stores the metrics per request id
    self.n_nodes = len(nodes)
    self.received_count = 0

  def run():
    output("Nodes:", nodes)
    failure_injected = False

    for tid in range(num_requests):
      metrics[tid] = RequestMetrics(time.clock(), time.time())
      n = random.sample(nodes,1)
      output("sent request: ", tid, " to node: ", n)
      send(('request', (tid,os.urandom(msg_size))), to=n)

      # Uncomment to use sim as an external failure detector
      # External failure detector
      if test_failure and not failure_injected and tid == num_requests/2:  ## failing last node
        send(('failure', 0), to= nodes[1])  ## send index of the last node as a failed node to all nodes
        send(('failure', len(nodes)-2), to= nodes[1])
        output("Sent failure message to first node")
        failure_injected = True
    output('done sending requests to the derecho system')

    while True:
      if await (len(setof(tid, received(('response', tid)))) == num_requests):
        output("---- ---- all responded")
        send(('metrics', list(metrics.values())), to= parent())
        break 
      elif timeout(5):
        output("---- ---- timeout!!!")
        for tid in range(num_requests):
          if not metrics[tid].success:
            n = random.sample(nodes,1)
            output('sending message ', tid, ' to node: ', n)
            send(('request', (tid,os.urandom(msg_size))), to=n)
    send('done', to=parent())
   
  def receive(msg= ('response', tid), from_= p):
    if not metrics[tid].success:
      cur_metric = metrics[tid]
      cur_metric.cpu_end_time = time.clock()
      cur_metric.run_end_time = time.time()
      cur_metric.success = True
      received_count += 1
      output("---- Received tid = ", tid, " from ", p, " with received_count: ", received_count)      ## Print the response received from the system

## take the number of nodes as input parameters during runtime, default
def main():
  config(channel is fifo, clock is lamport, visualize is {
        # colors: override message and process colors, defaults to random (to fix random?)
        #         supports any valid CSS color value
        # https://developer.mozilla.org/en-US/docs/Web/CSS/color_value
        # examples: Transparent, Yellow, DarkRed, rgb(255, 255 ,0),
        #           rgba(255, 255, 0, 0.1), hsl(210, 100%, 50%)
        'colors': {
            # processes
            #'Sim': 'aquamarine',
            # messages
            'request': 'purple',
            'response': 'blue',
            'data': 'lime',
            'control': 'red',
#           'rdma_write_sst': 'gray',
            'rdma_write_view': 'gray'
        }
    }, handling = 'all')

  num_nodes = int(sys.argv[1]) if len(sys.argv) > 1 else 5     ## number of nodes in the system
  num_requests = int(sys.argv[2]) if len(sys.argv) > 2 else 100 ## number of requests from applications
  nreps = int(sys.argv[3]) if len(sys.argv) > 3 else 1         ## number of repetitions to calculate the metrics
  msg_size = int(sys.argv[4]) if len(sys.argv) > 4 else 1      ## size of the message sent to the derecho system
  window_size = 5                                               ## window_size, should be small
  test_failure = False                                          ## set to True to test a failure scenario

  throughputs = []
  while nreps > 0:
    t1 = time.time()
    nodes = sorted(list(new(Node, num= num_nodes)))    ## create Node processes
    sim = new(Sim, (nodes, num_requests, test_failure, msg_size))  ## create and set up crash and client simulator process
    output("type of nodes is: ", type(nodes))
    for (rank, node) in enumerate(nodes): ## setup Node processes
      setup(node, (nodes, rank, window_size, sim))
    start(nodes)                          ## start Node processes
    start(sim)                            ## start the sim process
    await(received(('done'), from_ =sim)) ## wait to receive 'done' from the sim process
    throughputs.append((time.time() - t1)/num_requests)
    print("--------- time:", time.time() - t1)
    end(nodes)
    end(sim)
    nreps -=1

  cpu_times = []
  run_times = []
  failures = 0
  total_runs = 0
  for metrics in listof(metric, received(("metrics", metric))):
    for metric in metrics:
      total_runs += 1
      if metric.success:
        cpu_times.append(metric.cpu_end_time - metric.cpu_start_time)
        run_times.append(metric.run_end_time - metric.run_start_time)
      else:
        failures += 1

  output("AVG cpu_times", round(sum(cpu_times)/len(cpu_times), 5))
  output("AVG run_times", round(sum(run_times)/len(run_times), 5))
  output("AVG throughput", round(sum(throughputs)/len(throughputs), 5))
  output("FAILURES", failures)
