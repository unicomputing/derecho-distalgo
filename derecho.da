import sys
import random

# Derecho: Fast State Machine Replication for Cloud Services
# http://www.cs.cornell.edu/ken/derecho.pdf
# Implementation of the View Change Protocol as listed under section A.4 
# Pages 35 to 37


'''
A list of SSTRows form a Shared State Table (SST)
A copy of the SST is stored by each Node

getattr(x, 'foobar') is equivalent to x.foobar.   setattr(x, 'foobar', 123) is equivalent to x.foobar = 123.
'''
class SSTRow:
	def __init__(self, nprocess):
		self.suspected = [False] * nprocess
		self.proposal = [0] * nprocess
		self.num_committed = 0
		self.num_acked = 0
		self.nreceived = [0] * nprocess
		self.wedged = False
		self.changes = []
		self.num_changes = 0
		self.num_installed = 0

	def __str__(self):
		return " " + suspected

'''
A view holds the metadata of an epoch
'''
class View():
	def __init__(self, nprocess, leader_rank=0):
		self.max_rank = nprocess
		self.leader_rank = 0
		self.failed = [False] * nprocess
		self.wedged = False
		self.members = [None] * nprocess


class Node(process):
	'''
	- Simulate RDMA based write to an SST
	- Every Node owns a row in the SST and is the only writer 
	- Every local update to a cell has to be multicasted to all other Nodes to update their copy of the row
	- Derecho uses RDMA to update across Nodes, we simulate this uing the below functions write_sst and write_view
	- Every update to a cell is written locally and then multicasted to all Nodes for them to update their local copies
	'''
	def write_sst(row, attr, val, index=None):
		#update the local SST
		if index is not None:
			newval = getattr(sst[row], attr)
			newval[index] = val
			setattr(sst[row], attr, newval) 
		else:
			setattr(sst[row], attr, val) 
		message =  ('rdma_write_sst', row, attr, val, index)
		# output("sending message: " , message, " to ", (nodes-set([self])))
		# Cast the update message to all the Nodes
		send(message, to=(nodes-set([self])))

	#TODO implement across usecases
	def write_view(attr, val, index=None):
		#update the local view
		if index is not None:
			newval = getattr(curr_view, attr)
			newval[index] = val
			setattr(curr_view, attr, newval) 
		else:
			setattr(curr_view, attr, val) 
		# Cast the update message to all the Nodes
		send(('rdma_write_view', attr, val, index), to=(nodes-set([self])))

	'''
	- A reducer function
	- Calculates the min value for a columnn for non-failed Nodes in an SST
	- Column to be passed an a paramter
	'''
	def min_not_failed(attr):
		vals = set()
		for i in range(len(sst)):
			#output("MIN SST ", sst)
			if not sst[my_rank].suspected[i]:
				vals.add(getattr(sst[i], attr))
		if len(vals) == 0:
			return None
		else:
			return min(vals)

	'''
	- A reducer function
	- Calculates the the logical AND of all values in a columnn for non-failed Nodes in an SST
	- Column to be passed an a paramter
	'''
	def logical_and_not_failed(attr):
		val = True
		for i in range(len(sst)):
			#output("MIN SST ", sst)
			if not sst[my_rank].suspected[i]:
				val = val and getattr(sst[i], attr)
		return val

	def Count(attr, val):
		count = 0
		for i in range(len(sst)):
			if getattr(sst[i], attr) == val:
				count += 1 
		return count


	def setup(nodes, nprocess, my_rank):
		self.sst = [SSTRow(nprocess)]*nprocess
		self.sent_num = -1
		self.curr_view = View(nprocess)
		write_view('members', self, my_rank)

	def run():
		output("my_rank: ", str(my_rank))
		while True:
			if await(received(('end'))):
				break
			elif await((sst[curr_view.leader_rank].num_changes > sst[my_rank].num_acked) and my_rank != curr_view.leader_rank):			# A.4.2 when (sst[leader_rank].num_changes > sst[my_rank].num_acked); if(curr_view.leader_rank != my_rank)
				#output("to pro ", sst[curr_view.leader_rank].num_changes , " > " ,  sst[my_rank].num_acked)
				new_proposal()
			elif await((my_rank == curr_view.leader_rank) and (min_not_failed('num_acked') > sst[my_rank].num_committed)): 				# A.4.2 when (curr_view.leader_rank == my_rank and MinNotFailed(sst[∗].num_acked) > sst[my_rank].num_committed) 
				commit_proposal_leader()
			elif await(sst[curr_view.leader_rank].num_committed > sst[my_rank].num_installed): 											# A.4.2 when (sst[my_rank].num_committed[leader_rank] > sst[my_rank].num_installed[my_rank]) 
				install_new_view()
			elif timeout(7): 																											# IMPROV
				send(('end'), to=parent())

		#output("SST: ", sst)
		output("view: ", curr_view.members)
		output('terminating NODES')

	
	def report_failure(r): 															# A.4.1.
		output("in report failure")		
		#local node suspects node that owns the row report							
		write_sst(my_rank, 'suspected', True, r)									# sst[my_rank].suspected[r] = true;
		total_failed = Count('suspected', True) 									# total_failed = Count(sst[∗].suspected, true);
		if total_failed >= (nprocess + 1)/2:										# if (total_failed >= (num_members + 1)/2) {
			output("throw derecho_partitioning_exception")							# throw derecho_partitioning_exception; }
			return
		suspect() # IMPROV

	'''
	A.4.1
	returns the node that r believes to be the leader, on the basis of the current sst
	'''
	def find_new_leader(my_rank):													
		for i in range(len(sst)):													# for(int i = 0; i < curr_view.max_rank; ++i)
			if sst[my_rank].suspected[i]:											# if(sst[r].suspected[i])
				continue															# 	continue;
			else:																	# else
				return i 															# 	return i
	
	'''
	A.4.1
	wait until all non-suspected nodes comsider me to be the leader.
	implies that the new leader takes action only after every healthy node has
		pushed final nReceived data to it
	'''
	def leader_selection():
		new_leader = find_new_leader(my_rank)																# new_leader = find_new_leader(my_rank)
		if new_leader != curr_view.leader_rank and new_leader == my_rank:									# if(new_leader != curr_view.leader_rank && new_leader == my_rank)
			all_others_agree = True 																		# bool all_others_agree = true
			#so as long as I continue to believe I will be leader
			while find_new_leader(my_rank) == my_rank:														# while(find_new_leader(my_rank) == my_rank)
				for r in range(len(sst)):																	# for(r: SST.rows){
					if not sst[my_rank].suspected[r]:														# 	if(sst[my_row].suspected[r] == false)
						all_others_agree = all_others_agree and (find_new_leader(r) == my_rank)				# 		all_others_agree &&= (find_new_leader(r) == my_rank)
				
				if all_others_agree: # if(all_others_agree)
					#Scan sst to learn prior proposals
					write_view('leader_rank', my_rank)														# curr_view.leader_rank = my_rank
					break																					# 	break

	def suspect():
		for r in range(len(sst)): 																			# for(every row r and s)
			for s in range(len(sst)):
				if sst[r].suspected[s]: 																	# 	if(sst[r].suspected[s] == true)
					# failure propagation, local node also suspects s
					# and updates it SST row
					write_sst(my_rank, 'suspected', True, s) 												# 		sst[my_rank].suspected[s] = true

		for s in range(len(sst)): # for(s=0; s < num_members; ++s)
			# if s is newly suspected
			if sst[my_rank].suspected[s] and (not curr_view.failed[s]): 									# sst[my_rank].suspected[s] == true and curr_view.failed[s] == false
																											# freeze(s)
																											# report_failure(s)
				# mark s as failed in the current view
				curr_view.failed[s] = True 																	# curr_view.failed[s] = true
				curr_view.wedged = True 																	# curr_view.wedge()
				write_sst(my_rank, 'wedged', True) 															# sst[my_rank].wedged = true

				leader_selection() 																			# IMPROV

				if curr_view.leader_rank == my_rank and (s not in sst[my_rank].changes)	: 					# if(curr_view.leader_rank == my_rank and sst[my_rank].changes.contains(s) == false)
					next_change_index = sst[my_rank].num_changes - sst[my_rank].num_installed 				# next_change_index = sst[my_rank].num_changes − sst[my_rank].num_installed;
					#not required in python for position
					changes = sst[my_rank].changes 															# sst[my_rank].changes[next_change_index] = s #id of node owning S
					changes.append(s) 
					write_sst(my_rank, 'changes', changes) 
					num_changes = sst[my_rank].num_changes + 1 												# sst[my_rank].num_changes++;
					write_sst(my_rank, 'num_changes', num_changes)
					#proposed a new membership change and wedged the current view

		

	'''
	calculate next view membership
	'''
	def terminate_epoch():
		leader_rank = curr_view.leader_rank
		committed_count = sst[leader_rank].num_committed - (sst[leader_rank].num_installed)					# committed_count = sst[leader_rank].num_committed − sst[leader_rank].num_installed;
		sst[my_rank].num_installed = sst[leader_rank].num_committed											# IMPROV
		next_view = View(nprocess - len(sst[my_rank].changes))
		next_view.leader_rank = curr_view.leader_rank
		next_view.members = []
		for n in range(len(curr_view.members)): 															# for (change_index = 0; change_index < committed_count; change_index++) {
			#																								# node_id = sst[my_rank].changes[change_index]; 
			#																								# if node already a member, the change is to remove the node;
			#
			if n not in sst[my_rank].changes: 																# if (curr_view.contains(node_id) == true) {
				next_view.members.append(curr_view.members[n]) 												# next_view.members = curr_view.members; # new_view.members.remove(node_id);
			# 	MISSING																						# otherwise the change is to add the node
			# 	MISSING																						# else {
			# 	MISSING																						# next_view.members.append(node_id);
		if leader_rank == my_rank: 																			# if (leader_rank == my_rank) {
			output("leader_ragged_edge_cleanup ") 															# leader_ragged_edge_cleanup();
		else: # else
			output("non_leader_ragged_edge_cleanup()") 														# when (sst[leader_rank].ragged_edge_computed == true) { non_leader_ragged_edge_cleanup(); }
		curr_view = next_view 																				# curr_view = next_view;
		#new view installed

	def new_proposal():
		output("leader proposed a new change") 																# |= leader proposed a new change
		leader_rank = curr_view.leader_rank	
		write_sst(my_rank, 'num_acked', sst[leader_rank].num_changes) 										# IMPROV
		write_sst(my_rank, 'num_changes', sst[leader_rank].num_changes) 									# sst[my_rank].num_changes = sst[leader_rank].num_changes;
		# copy entire changes vector from the leader's row
		write_sst(my_rank, 'changes', sst[leader_rank].changes) 											# sst[my_rank].changes = sst[leader_rank].changes;
		write_sst(my_rank, 'num_committed', sst[leader_rank].num_committed) 								# sst[my_rank].num_committed = sst[leader_rank].num_committed;
		curr_view.wedged = True 																			# curr_view.wedge();
		write_sst(my_rank, 'wedged', True) 																	# sst[my_rank].wedged = true;
		# acknowledged leader's proposal and wedged the current view


	def commit_proposal_leader():
		#acknowledge a new proposal 																		# |= K U\F ( acknowledged a new proposal )
		output("commit_proposal_leader")
		write_sst(my_rank, 'num_committed', min_not_failed('num_acked')) 									# sst[my_rank].num_committed = MinNotFailed(sst[∗].num_acked);
		#commited acknowledged proposals



	def install_new_view():
		# leader committed a new membership  # 22 |= leader committed a new membership change
		curr_view.wedged = True 																			# curr_view.wedge();
		write_sst(my_rank, 'wedged', True) 																	# sst[my_rank].wedged = true;
		await(logical_and_not_failed('wedged'))																# when (LogicalAndNotFailed(sst[∗].wedged) == true) {
		output("terminate epoch") 																			# / |= K U\F ( current view is wedged)
		terminate_epoch() 																					# terminate_epoch


	'''
	Receives a failure trigger
	This is to simulate: polling on the SST table for not receving a message for a time-period 
		from one of the servers

	A.4.1 Failure handling and leader proposing changes for next view.

	'''
	def receive(msg= ('failure', r)):											# every 1 millisecond post RDMA write with completion time to every SST Row that is not frozen
		if r != my_rank:														# if (no completion polled from row r)
			output("Received failure of ", r)									# sst.freeze(r)
			report_failure(r)													# report_failure(r)
	

	def receive(msg = ('rdma_write_sst', row, attr, val, index)):
		#write_sst(row, attr, val, index, False)
		#update the local SST
		output("Received message:" , ('rdma_write_sst', row, attr, val, index))
		if index is not None:
			newval = getattr(sst[row], attr)
			newval[index] = val
			setattr(sst[row], attr, newval) 
		else:
			setattr(sst[row], attr, val) 

	def receive(msg = ('rdma_write_view', attr, val, index)):
		#update the local view
		output("Received message:" , ('rdma_write_view', attr, val, index))
		if index is not None:
			newval = getattr(curr_view, attr)
			newval[index] = val
			setattr(curr_view, attr, newval) 
		else:
			setattr(curr_view, attr, val) 

class Sim(process):
	def setup(nodes):
		pass
	
	def run():
		output(nodes)
		#failing last node
		send(('failure', len(nodes)-1), to= nodes)
		output("Sent failure message to nodes")
		await(received(('end')))
		output('terminating SIM')


def main():
	nprocess = int(sys.argv[1]) if len(sys.argv) > 1 else 1

	#TODO start View
	nodes = new(Node, num= nprocess)
	rank = 0
	for n in nodes:
		setup(n, (nodes, nprocess,rank,))
		rank += 1
	start(nodes)
	sim = new(Sim, (nodes,), num=1)
	start(sim)
	await(len(setof(a, received(('end'), from_ =a))) == len(nodes) - 1)
	#each(n in nodes, has=received(('end'), from_=n)))
	send(('end'), to=(sim|nodes))
	#await(each(s in (sim|nodes), has=received(('end',), from_=s)))



# NOT IMPLEMENTED
'''
A.3.1 Receive.

on recv(M(i,k)) {
	// |= K me (Received M(i,k))
	// store the message to deliver later
	msgs[дi(M(i,k))] = M(i,k);
	sst[my_rank].latest_received_index[i ] = k;
	// calculate global index of the message in the global round−robin ordering
	(min_index_received, lagging_node_rank) =
	(min,argmin) i sst[my_rank].latest_received_index[i];
	sst[my_rank].global_index = (min_index_received + 1) ∗ |G| + lagging_node_rank − 1;
	// |= K me (Received all messages M(i,k) s.t. дi(M(i,k)) ≤ sst[my_rank].global_index)
}

A.3.2 Stability.

always {
	sst[my_rank].stable_msg_index = Min{sst[∗].global_index}
	// |= K me (∀p ∈ G : K p (Received all messages M(i,k) s.t. дi(M(i,k)) ≤ sst[my_rank].global_index))
}

A.3.2 Delivery.

always {
	min_stable_msg_index = Min{sst[∗].stable_msg_index}
	// |= K me (^K 2 (“Received all messages M(i,k) s.t. дi(M(i,k)) ≤ sst[my_rank].global_index”))
	for (msg : msgs) {
		if (msg.global_index <= min_stable_msg_index) {
			deliver_upcall(msg);
			msgs.remove(msg.global_index);
		}
	}
	sst[my_rank].latest_delivered_index = min_stable_msg_index
}
// |= K me (Delivered all messages ≤ sst[my_rank].latest_delivered_index)

A.4.2
leader_ragged_edge_cleanup() {
	if (LogicalOr(sst[∗].ragged_edge_computed) == true) {
		Let rank be s.t. sst[rank].ragged_edge_computed is true
		// copy min_latest_received from the node that computed the ragged edge
		for (n = 0; n < |G|; ++n) {
			sst[my_rank].min_latest_received[n] = sst[rank].min_latest_received[n];
		}
		sst[my_rank].ragged_edge_computed = true;
	}
	else {
		for (n = 0; n < |G|; ++n) {
		sst[my_rank].min_latest_received[n] = Min(sst[∗].latest_received_index[n]);
		// |= K me ( sst[my_rank].min_latest_received[n] number of messages from n are safe for delivery)
		}
		sst[my_rank].ragged_edge_computed = true;
	}
	deliver_in_order();
}

non_leader_ragged_edge_cleanup() {
	// copy from the leader
	for (n = 0; n < |G|; ++n) {
		sst[my_rank].min_latest_received[n] = sst[leader_rank].min_latest_received[n];
	}
	sst[my_rank].ragged_edge_computed = true;
	deliver_in_order();
}

deliver_in_order() {
	curr_global_index = sst[my_rank].latest_delivered_index;
	max_global_index = max over n of (sst[my_rank].min_latest_received[n] ∗ |G| + n);
	for (global_index = curr_global_index + 1; global_index <= max_global_index; ++global_index) {
		sender_index = global_index / |G|;
		sender_rank = global_index % |G|;
		if (sender_index <= sst[my_rank].min_latest_received[sender_rank]) {
			deliver_upcall(msgs[global_index]);
		}
	}
}
'''