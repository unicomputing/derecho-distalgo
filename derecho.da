## This is a DistAlgo implementation of Derecho, as described in
## Derecho: Fast State Machine Replication for Cloud Services, by
## SAGAR JHA, Cornell University, USA
## JONATHAN BEHRENS, Cornell University, USA and MIT, USA
## THEO GKOUNTOUVAS, MATTHEW MILANO, WEIJIA SONG, EDWARD TREMEL,
## ROBBERT VAN RENESSE, SYDNEY ZINK, and KENNETH P. BIRMAN,
## Cornell University, USA
## ACM Transactions on Computer Systems, Vol. 36, No. 2. Article 4. March 2019.
## http://www.cs.cornell.edu/ken/derecho.pdf
##
## Implementation of the View Change Protocol as listed in 
## Appendix A: Pseudo-code for key protocol steps.  Pages 32 to 37.

import sys
import random
import time
import os

class Slot:
  """a slot that store a message"""

  # A.2.1 SST Structure. SMC uses two fields, slots and received_num. 
  # slots is a vector of window_size slots, 
  # each of which can store a message of up to max_message_size characters. 
  # The index associated with a slot is used to signal that 
  # a new message is present in that slot: 
  # For example, if a slot's index had value k and transitions to k + 1, 
  # then a new message is available to be received.
  # The vector received_num holds 
  # counters of the number of messages received from each node.
  def __init__(self):
    self.index = 0   ## index position of this slot in vector of slots
    self.buf = None  ## message in this slot, of up to max_message_size characters
    self.size = 0    ## size of message in this slot, up to max_message_size

class SSTRow:  ## The variables and their initial values are not explicitly mentioned in the paper
  """a row in the SST: shared state table"""

  # Sec 3.4 Shared State Table: The SST. 2nd paragraph, Lines 1-6:
  # Derecho uses protocols that run on a novel replicated data structure 
  # called the shared state table, or SST. 
  # The SST offers a tabular distributed shared memory abstraction. 
  # Every member of the top-level group holds 
  # its own replica of the entire table, in local memory.
  # Within this table, there is one identically formatted row per member. 
  # A member has full read/write access to its own row but
  # is limited to read-only copies of the rows associated with other members.
  # Page 12, Sec 3.4 The SST offers a tabular distributed shared memory abstraction. 
  # Every member of the top-level group holds its own replica of the entire table, in local memory. (source: 3.4 Shared State Table: The SST, paragraph 2, line 4)
  # Within this table, there is one identically formatted row per member. A member has full read/write access to its own row 
  # but is limited to read-only copies of the rows associated with other members
  ## In this implementation a list of SSTRows objects form a Shared State Table (SST)
  ## A copy of the SST is stored by each Node.
  #
  # A.1.1 SST.
  # column_name->string|string[int] // e.g. wedged or latest_received_index[3]
  # sst_row->sst[row_rank]
  # row_rank->int
  # sst_column->sst[*].column_name
  # sst_entry->sst_row.column_name // e.g. sst[0].stable_msg_index[0]
  def __init__(self, num_nodes, window_size):
    ## each field is a column name.
    ## if field is a LIST, it stores a value for each other node based on the index of the node.
    self.suspected = [False] * num_nodes ## LIST; for each node, whether the node is suspected to be crashed
    self.num_committed = 0     ## number of changes committed by the current node, as suggested by the Leader
    self.num_acked = 0         ## number of changes acknowleged by the current node, as suggested by the Leader
    self.received_num = [-1] * num_nodes ## LIST; for each node, number of messages received by the current node
    self.wedged = False        ## whether the current node has wedged its SST
    self.changes = []          ## list of node ids of crashed/new nodes
    self.num_changes = 0       ## number of changes, i.e., length of self.changes
    self.num_installed = 0     ## number of `changes` applied or installed to the current node's SST
    self.min_latest_received = [-1] * num_nodes    ## LIST; for each node, minimum index of message received from the node
    self.latest_received_index = [-1] * num_nodes  ## LIST; for each node, index of latest message received from the node
    self.ragged_edge_computed = True     ## whether the ragged trim is finalized for this node after a failure 
    self.latest_delivered_index = -1     ## index of the last message successfully delivered by the current node to all nodes
    self.global_index = -1               ## global message index counter, used for total ordering of messages in the system
    self.slots = [Slot() for x in range(window_size)]  ## each Slot stores the new message recieved by the current node
    ### ?? The pseudocode and code is missing the final field

class View(): 
  """ ## A view holds the metadata of an epoch""" ## Understood from Fig.11 page 17
  ### ?? Does it not contain epoch number?
  def __init__(self, num_nodes, leader_rank=0, epoch=0):
    self.leader_rank = leader_rank        ## rank of the leader in the current view
    self.failed = [False] * num_nodes     ## true against index of each node's rank if a process is deemed 'failed' in the view
                                          ## (for each node, whether the node is deemed 'failed' in the current view)
    self.wedged = False                   ## whether the view is 'wedged' by the system
    self.members = [None] * num_nodes     ## list of members (nodes) in the current view
    self.epoch = epoch                    ## Current epoch of the system

  def add_member(self, node_id):          ## Add member to the current view
    self.members.append(node_id)          ## append the member/node to the list of members
    self.failed.append(False)             ## add the failed attribute corresponding to the removed node

  def remove_member(self, node_id):       ## Remove member to the current view
    try:
      index = self.members.index(node_id) ## get the index of the member/node to be deleted
      self.members.remove(node_id)        ## remove the member/node to the list of members
      del self.failed[index]              ## remove the failed attribute corresponding to the removed node
    except Exception as e:
      output("Exception occured while removing member: ", node_id, " from the view. ", e)
      return

class Node(process):

  ## Simulate RDMA based write to an SST
  ## Every Node owns a row in the SST and is the only writer. 
  ## Every local update to a cell must be multicasted to all other Nodes to update their copy of the same row.
  ## Derecho uses RDMA to update across Nodes, we simulate this using the below functions write_sst and write_view.
  ## Every update to a cell is written locally and then multicasted to all Nodes for them to update their local copies.
  ## row: row number in the SST.
  ## col: column name to be updated.
  ## val: value with which the column has to be updated.
  ## index: (optional) if the column is a list, pass the index in the list to be updated
  def write_sst(row, col, val, index=None):  ## update the local SST
    if index is not None:                    ## check if a non null 'index' is passed, implies updating a list
      newval = getattr(sst[row], col)        ## retrieve the current list into newval
      newval[index] = val                    ## update the value at index to the val passed
      setattr(sst[row], col, newval)         ## update the local sst row with newval
    else:
      setattr(sst[row], col, val)            ## if it's not a list, just set the value to the new val passed
    message = ('rdma_write_sst', row, col, val, index)  ## create an update message to broadcast the update to others nodes
    message_tagged = ('data' if message[2] == "slots" else 'control',) + message ## new for separating data and control
    output("====", message_tagged)
    send(message_tagged, to=(nodes-{self}))  ## send the update message to other nodes, so they update their local copies

  ### ?? Remove if not needed
  ## Simulate RDMA based write to a View  (possibly merge with previous def?)
  ## Similar to how write_sst is implemented 
  ## Takes the column, value and index and update the view
  ## col: field name to be updated
  ## val: value with which the field has to be updated
  ## index: (optional) if the field is a list, pass the index in the list to be updated
  def write_view(col, val, index=None):      ## update the local view
    if index is not None:                    ## check if a non null 'index' is passed, implies updating a list
      newval = getattr(curr_view, col)       ## retrieve the current list into newval
      newval[index] = val                    ## update the value at index to the val passed
      setattr(curr_view, col, newval)        ## set the newval to the local view object
    else:                               
      setattr(curr_view, col, val)           ## if its not a list, just set the value to the new val passed
    send(('rdma_write_view', col, val, index), to=(nodes-{self}))
                                             ## send update message to other nodes, so they update their local copies of the view

  ## The following aggregation functions are not defined in the paper.
  def min_not_failed(col):
    """min value of column named col for non-suspected nodes in the sst of this node"""
    ### ?? Remove
    output("in min_not_failed function")
    for row in range(len(sst)):
      if not sst[my_rank].suspected[row]:
        output("value of this col is: ", getattr(sst[row], col))
    return min({getattr(sst[row], col) for row in range(len(sst)) if not sst[my_rank].suspected[row]})

  def min_(col, index = None):
    """min value of column col, at given index for col being a list, in the sst of this node"""
    try:
      if index is None:
        return min({getattr(sst[row], col) for row in range(len(sst)) if getattr(sst[row], col) >= 0})
      else:
        return min({getattr(sst[row], col)[index] for row in range(len(sst)) if getattr(sst[row], col)[index] >= 0})
    except:
      return -1

  def logical_and_not_failed(col):
    """logical 'AND' of all values in column col for non-suspected nodes in the sst of this node"""
    return all(getattr(sst[row], col) for row in range(len(sst)) if not sst[my_rank].suspected[row])

  def logical_or(col):
    """logical 'OR' of all values in column col in the sst of this node"""
    return any(getattr(sst[row], col) for row in range(len(sst)))

  def min_with_val(col, val):
    """minimum rank of row in the sst of this node where column col has value val"""
    return min({row for row in range(len(sst)) if getattr(sst[row], col) == val})

  def Count(col, val):
    """count of rows with value val for column col in the sst of this node"""
    return len({row for row in range(len(sst)) if getattr(sst[row], col) == val})

  def get_max_gi():
    """max value of the global index of all the messages in the system,
       max of (sst[my_rank].min_latest_received[n] ∗ |G| + n) over n in 1..|G|"""
    if len(G) < 1: return 0
    return max((sst[my_rank].min_latest_received[n] * len(G) + n) for n in range(len(G)))

  # A.1.2 Message Ordering
  # Message: M(i, k) represents a message with i as the sender rank and 
  # k as the sender index.
  # For example, the zeroth message by sender number 2 is M(2, 0). 
  # We have a round-robin ordering imposed on messages. 
  # M(i1, k1) < M(i2, k2) <==> k1 < k2 ||(k1 == k2 and i1 < i2).
  # The global index of M(i, k), gi(M(i, k)) is the position of this message
  # in the round-robin ordering. 
  # So M(0, 0) has a global index of 0, M(1, 0) has a global index of 1, 
  # and so on
  def gi(i, k):
    """global index of a message given sender node's rank i and sender's index k of the message""" 
    return (k) * len(G) + i               # gi(M(i, k)) = i + |G| * k

  # A.2.2 Initialization.
  def setup(nodes, my_rank, window_size, sim):
                                          # for i in 1 to n {
                                          #   for j in 1 to n {
                                          #     sst[i].received_num[j] = -1; }
                                          #   for k in 1 to window_size {
                                          #     sst[i].slots[k].buf = nullptr
                                          #     sst[i].slots[k].index = 0 }}
    self.num_nodes = len(nodes)           ## number of nodes in the system
    self.sst = [SSTRow(num_nodes, window_size) for _ in range(num_nodes)]   ## initialized the local SST for this node
    self.sent_num = -1                    ## sent_num = -1   ## helper variable to maintain a count of messages sent by this node
    self.curr_view = View(num_nodes)      ## initialize the local view object
    self.G = nodes                        ## the set of nodes in the system
    self.msgs = {}                        ## dictionary, messages sent/received by this node, indexed by the global index of each message
    self.max_msg_size = 10                ## limit on the maximum message size for the system
    for i, node in enumerate(nodes):
      self.curr_view.members[i] = node    ## initially updating the 'members' list of the view
    self.results = []                     ## store the result/messages processed in the upcall-call
    ### ? Should we maintain the results per client per request number?
    output('initial nodes configuration: ', nodes)

  def run():
    while True:
      # A.4.2 Terminating old view and installing new view after wedging.
      if await(sst[curr_view.leader_rank].num_changes > sst[my_rank].num_acked # when (sst[leader_rank].num_changes > sst[my_rank].num_acked) {
               and curr_view.leader_rank != my_rank):  
        output("leader proposed a new change")                               #   // |= leader proposed a new change
                                                                             #   if(curr_view.leader_rank != my_rank) {
        leader_rank = curr_view.leader_rank  
        write_sst(my_rank, 'num_acked', sst[leader_rank].num_changes)        ## missing but is needed to terminate; acknowledge the changes
        write_sst(my_rank, 'num_changes', sst[leader_rank].num_changes)      #     sst[my_rank].num_changes = sst[leader_rank].num_changes;
                                                                             #     // copy entire changes vector from the leader's row
        write_sst(my_rank, 'changes', sst[leader_rank].changes)              #     sst[my_rank].changes = sst[leader_rank].changes;
        write_sst(my_rank, 'num_committed', sst[leader_rank].num_committed)  #     sst[my_rank].num_committed = sst[leader_rank].num_committed;
        curr_view.wedged = True                                              #     curr_view.wedge(); ### ?? How? Should you add functionality to freeze etc?
        write_sst(my_rank, 'wedged', True)                                   #     sst[my_rank].wedged = true;
                                                                             #     // |= acknowledged leader's proposal and wedged the current view
                                                                             #   }}
      elif (curr_view.leader_rank == my_rank and                             # when (curr_view.leader_rank == my_rank and 
            min_not_failed('num_acked') > sst[my_rank].num_committed):       #       MinNotFailed(sst[∗].num_acked) > sst[my_rank].num_committed) {
        output("commit_proposal_leader")                                     #   // |= K U\F ( acknowledged a new proposal )
        write_sst(my_rank, 'num_committed', min_not_failed('num_acked'))     #   sst[my_rank].num_committed = MinNotFailed(sst[∗].num_acked);
                                                                             #   // |= commited acknowledged proposals
                                                                             # }
      elif sst[curr_view.leader_rank].num_committed > sst[my_rank].num_installed:
                                                 # when (sst[my_rank].num_committed[leader_rank] > sst[my_rank].num_installed[my_rank]) {
                                                 #   // |= leader committed a new membership change
        output ('leader committed a new membership change')
        curr_view.wedged = True                  #   curr_view.wedge();
        write_sst(my_rank, 'wedged', True)       #   sst[my_rank].wedged = true;
        await(logical_and_not_failed('wedged'))  #   when (LogicalAndNotFailed(sst[∗].wedged) == true) {
                                                 #     // |= K U\F (current view is wedged)
        terminate_epoch()                        #     terminate_epoch(); }}
      elif timeout(0.1):      ## runs the below message protocols always
        --receive_pending_msg
        suspect()             # always
        --receive_pending_msg
        receive_msg()         # always 
        --receive_pending_msg
        stability_delivery()  # always
        --receive_pending_msg

  # A.2.3 Sending. First the sending node reserves one of the slots:
  ## resolve the available slot before sending the message
  def get_buffer(msg_size):                       # char* get_buffer(msg_size) { ## returns true if slot is available
    output("in get_buffer function")
    if msg_size > max_msg_size: return False      #   assert(msg_size <= max_msg_size);
                                                  #   // A Slot can be reused if the previos message in that slot was received by everyone
                                                  #   // Combine it with the FIFO ordering of messages
    completed_num = min_("received_num", my_rank) #   completed_num = Min{sst[*].received_num[my_rank]};
    if sent_num - completed_num >= window_size:   #   if (sent_num - completed_num > window_size) { ## modified to avoid deadlock (>= was added in the condition)
      output("slot buffer seems to be full, returning, sent-num: ", sent_num, " completed-num: ", completed_num)
      return False                                #     return nullptr; } ## modified, returns False to signify failure 
    slot = (sent_num + 1) % window_size           #   slot = (sent_num + 1) % window_size;
    sst[my_rank].slots[slot].size = msg_size      #   sst[my_rank].slots[slot].size = msg_size;
    return True                                   #   return sst[my_rank].slots[slot].buf; } ## modified, returns True to signify success 
  
  ## increment the slot and send the message to other nodes
  def send_msg(msg):
    output("in send_msg function, msg: ", msg)
    is_buffer_available = get_buffer(0) if msg is None else get_buffer(len(msg)) ## check if slot buffer is available
    if not is_buffer_available:                   #   // After get_buffer returns a non-null buffer, the application 
      return                                      #   // writes the message contents in the buffer and calls send
    slot = (sent_num + 1) % window_size           #   slot = (sent_num + 1) % window_size;
    sst[my_rank].slots[slot].buf = msg            #   the application writes the message contents in the buffer
    sst[my_rank].slots[slot].index += 1           #   sst[my_rank].slots[slot].index++;
    output("index for slot: ", slot, " is set to index: ", sst[my_rank].slots[slot].index, " and sent_num is: ", sent_num)
    write_sst(my_rank, "slots", sst[my_rank].slots[slot], slot)     
    sent_num += 1                                 #   sent_num++; }
  
  def receive(msg=('request', msg)):      ## receive request from application (Sim)
    send_msg(msg)                         ## send the message in the next slot

  # A.2.4 Receiving.
  ## receives a message, increments the current slot based on the window size and receives the message
  def receive_msg():                                       # always {
    output("in receive_msg function")
    for i in range(num_nodes):                             #   for i in 1 to n {
                                                           #    // the next message from node i will arrive in this slot
      slot = (sst[my_rank].received_num[i]+1)%window_size  #    slot = (sst[my_rank].received_num[i]+1)%window_size
      output('my_rank is: ', my_rank, ' node is: ', list(nodes)[i], ' index is: ', sst[i].slots[slot].index, ', sent_num is: ', sent_num, ' and received_num is: ', sst[my_rank].received_num[i])
      if sst[i].slots[slot].index == (sst[my_rank].received_num[i]+1)//window_size+1:
                                                           #    if(sst[si].slots[slot].index == (sst[my_rank].received_num[i]+1)/window_size+1) { 
        output('new message available for node: ', list(nodes)[i], 'in slot:', slot, ', at index: ', sst[i].slots[slot].index, ' and received_num: ', (sst[my_rank].received_num[i]+1))
        write_sst(my_rank, "received_num", sst[my_rank].received_num[i]+1, i)
                                                           #      ++sst[my_rank].received_num[i];
        recv((i, sst[my_rank].received_num[i]))            #      recv(M(i, sst[my_rank].received_num[i])); }}}

  ## minimun index received by this node and the lagging node according to this Node's SST
  def get_min_idx_rec():  # (min,argmin)i sst[my_rank].latest_received_index[i];   
    min_ind = min(sst[my_rank].latest_received_index[rank] for rank in range(len(sst[my_rank].latest_received_index)) if rank != my_rank)
    lagging_node = max(rank for rank in range(len(sst[my_rank].latest_received_index)) if sst[my_rank].latest_received_index[rank] == min_ind)  ## node from minimum latest index received is the lagging node as per this node's reference
    return min_ind,lagging_node

  # A.3 Atomic Multicast Delivery in the Steady State
  # A.3.1 Receive.
  ## Receives a message and adds it to the global message queue
  def recv(M):                                         # on recv(M(i,k)) {
    output("in recv function, message: ", M)           #   // |= K_me(Received M(i,k))
                                                       #   // store the message to deliver later
    (i, k) = M
    ### Seems like, it was added so as to avoid any holes in the slots
    if my_rank < i and gi(my_rank, k) not in msgs:     #  if(my_rank < i && I have not sent M(my_rank, k)) {
      output("Sending NULL message for", (my_rank, k))
      send_msg(None)                                   #  send a null message
    elif my_rank > i and k > 0 and gi(my_rank, k - 1) not in msgs:  # else if(my_rank > i && I have not sent M(my_rank, k-1)) {
      output("Sending NULL message for", (my_rank, k-1))
      send_msg(None)                                   # send a null message

    msgs[gi(i, k)] = (i, k)                            #   msgs[hi(M(i,k))] = M(i,k);
    write_sst(my_rank, "latest_received_index", k, i)  #   sst[my_rank].latest_received_index[i ] = k;    
                                                       #   // calculate global index of the message in the global round−robin ordering
    min_index_received, lagging_node_rank = get_min_idx_rec()
                                                       #   (min_index_received, lagging_node_rank) =
                                                       #       (min,argmin) i sst[my_rank].latest_received_index[i];  
    write_sst(my_rank, "global_index", (min_index_received + 1) * len(G) + lagging_node_rank - 1)
                                                       #   sst[my_rank].global_index = (min_index_received + 1) ∗ |G| + lagging_node_rank − 1;
                                                       #   // |= K_me (Received all messages M(i,k) s.t. дi(M(i,k)) ≤ sst[my_rank].global_index)
                                                       # }

  def deliver_upcall(msg, index=None):                 ### ? Remove index?
    output("in deliver_upcall function, msg: ", msg)
    if sst[msg[0]].slots[msg[1]%window_size].buf is not None:
      output(" deliver_upcall ", sst[msg[0]].slots[msg[1]%window_size].buf)       ## No implementation provided in the pseudocode
      results.append((index, sst[msg[0]].slots[msg[1]%window_size].buf))
      send(('response', sst[msg[0]].slots[msg[1]%window_size].buf[0]), to= sim)   ## Send the message back to Sim
      output("response sent to the client/sim, current results list is ", results)

  # A.3.2 Stability and Delivery
  ## Calculates the next global index for message delivery 
  def stability_delivery():                   # always {
    output("in stability_delivery function")
    stable_msg_index = min_("global_index")   #   stable_msg_index = Min{sst[∗].global_index}
                                              #   // |= K_me (∀p ∈ G : K p (Received all messages M(i,k) s.t. дi(M(i,k)) ≤ sst[my_rank].global_index))
    output("stable_msg_index", stable_msg_index)
    to_delete = []
    output("current msgs content: ", msgs)
    for i, msg in enumerate(msgs):            #   for (msg : msgs) {
      if msg <= stable_msg_index:             #     if (msg.global_index <= min_stable_msg_index) { ###  the text says "msg.global_index"? => msgs is a dictionary indexed by global index
        output("blah blah checking content of message ", msg, " and stable_msg_index ", stable_msg_index)
        deliver_upcall(msgs[msg], i)          #       deliver_upcall(msg);
        to_delete.append(msg)                 ##  store the messages that needs to be deleted, to delete later 
    for d in to_delete:
      del msgs[d]                             #   msgs.remove(msg.global_index); }} ###  msg is a tuple, here d is a tuple
    write_sst(my_rank, "latest_delivered_index", stable_msg_index)
                                              #   sst[my_rank].latest_delivered_index = stable_msg_index }
                                              # // |= K_me (Delivered all messages ≤ sst[my_rank].latest_delivered_index)
  
  # A.4 View Change Protocol
  # A.4.1 Failure Handling and Leader Proposing Changes for Next View.
  ## Receives a failure trigger.
  ## This simulates polling on SST for not receving a message for a time-period from one of the nodes
  def receive(msg=('failure', r)):              # every 1 millisecond {
                                                #   post RDMA write with completion time to every SST Row that is not frozen
    if r != my_rank: 				                    ## register node failure by only the non failing nodes
						                                    #   if (no completion polled from row r) {
      output("freeze")                          #     sst.freeze(r); ### ?? # no impl mentioned? Can we have a field checking if the sst is frozen or not
      output("Received failure of ", r)              
      report_failure(r)                         #     report_failure(r); }}

  ## update the suspected field upon noticing a node failure
  def report_failure(r):                              # report_failure(r) {
    output("in report_failure function")              #   // local node suspects node that owns the row r
    write_sst(my_rank, 'suspected', True, r)          #   sst[my_rank].suspected[r] = true;
    total_failed = Count('suspected', True)           #   total_failed = Count(sst[∗].suspected, true);
    if total_failed >= (num_nodes + 1)/2:             #   if (total_failed >= (num_members + 1)/2) {
      output("ERROR: derecho_partitioning_exception") #     throw derecho_partitioning_exception;
      return                                          #   }
                                                      # }

  def find_new_leader(rank):                # find_new__leader(r) {
                                            #   // returns the node that r believes to be the leader, on the basis of the current sst.
                                            #   // Notice that becasue of asynchrony in the sst update propagations, 
                                            #   // callers other than r itself might be using old version's of r's suspicion set, 
                                            #   // in which cse the caller could obtain an old leader belif of r's.
                                            #   // This is safe because leader beliefs are monotonic (they are ascending, in rank order).
    for i in range(len(curr_view.members)): #   for (int i = 0; i < curr_view.max_rank; ++i) {
      if sst[rank].suspected[i]: continue   #     if(sst[r].suspected[i]) continue;
      else: return i                        #     else return i }}

  ## update the view at the end of the current leader
  def leader_selection():                                              # always {
                                                                       # // Wait until all non-suspected nodes comsider me to be the leader.
                                                                       # // This implies that the new leader takes action only after
                                                                       # // every healthy node has pushed final nReceived data to it.
    output("in leader_selection function")
    new_leader = find_new_leader(my_rank)                              # new_leader = find_new_leader(my_rank)
    if new_leader != curr_view.leader_rank and new_leader == my_rank:  # if(new_leader != curr_view.leader_rank && new_leader == my_rank)
      all_others_agree = True                                          #   bool all_others_agree = true
                                                                       #   // so as long as I continue to believe I will be leader
      while find_new_leader(my_rank) == my_rank:                       #   while(find_new_leader(my_rank) == my_rank) {
                                                                       #     // check if everyone else agrees I am leader
        for r in range(len(sst)):                                      #     for(r: SST.rows) {
          if not sst[my_rank].suspected[r]:                            #       if(sst[my_row].suspected[r] == false)
            all_others_agree = all_others_agree and (find_new_leader(r) == my_rank)
                                                                       #         all_others_agree &&= (find_new_leader(r) == my_rank) }
        if all_others_agree:                                           #     if(all_others_agree) {
                                                                       #     // Scan sst to learn prior proposals
          self.curr_view.leader_rank = my_rank                         #       curr_view.leader_rank = my_rank;
          # write_view('leader_rank', my_rank)                         ### ?? Remove
          break                                                        #       break; }}}
    else:
      self.curr_view.leader_rank = find_new_leader(my_rank)

  ## poll the suspected field in the SST and propagate the status across the systems, page 36
  def suspect():                                         # always {
    output("in suspect function")
    for r in range(len(sst)):                            #   for(every row r and s) {
      for s in range(len(sst)):
        if sst[r].suspected[s] and not sst[my_rank].suspected[s]: #     if(sst[r].suspected[s] == true) { ### ? Should there be a condition to check if my_rank, suspected is false? will avoid message sends
                                                         #       // failure propagation---local node also suspects s
          write_sst(my_rank, 'suspected', True, s)       #       sst[my_rank].suspected[s] = true
                                                         #     }}
    for s in range(len(sst)):                            #   for(s=0; s < num_members; ++s) {
                                                         #     // if s is newly suspected
      if sst[my_rank].suspected[s] and (not curr_view.failed[s]):
                                                         #     if sst[my_rank].suspected[s] == true and curr_view.failed[s] == false {
        output('', s, ' is suspected')
                                                         #       freeze(s)  ### ? No implementation found?
        report_failure(s)                                #       report_failure(s)
                                                         #       // mark s as failed in the current view
        curr_view.failed[s] = True                       #       curr_view.failed[s] = true
                                                         #       // |= s in F
                                                         #       // removes predicates defined in section 1 so that no new message can be sent or delivered
        curr_view.wedged = True                          #       curr_view.wedge()
        ### ?? B.3 marks its SST row as wedged before pushing entire SST row to all non-failed
        ### top-level group members. Missing 'non-failed' check?
        write_sst(my_rank, 'wedged', True)               #       sst[my_rank].wedged = true
        leader_selection()                               ## adding a call for leader selection, to maintain sequence of actions

        if curr_view.leader_rank == my_rank and s not in sst[my_rank].changes:
                                                         #       if(curr_view.leader_rank == my_rank and sst[my_rank].changes.contains(s) == false) {
          node_id = list(nodes)[s]
          output("Removal of the node ", s, " with id: ", node_id, " proposed")
          # next_change_index = sst[my_rank].num_changes − sst[my_rank].num_installed; ## not required in python for position
          changes = sst[my_rank].changes                               
          changes.append(node_id) 
          write_sst(my_rank, 'changes', changes)         #         sst[my_rank].changes[next_change_index] = id of node owning s
          num_changes = sst[my_rank].num_changes + 1     #
          write_sst(my_rank, 'num_changes', num_changes) #         sst[my_rank].num_changes++;
          write_sst(my_rank, 'num_acked', num_changes)   ##        update the leader's (self) num_acked field to num_changes (for the membership execution condition to be true for leader)
                                                         #         // |= proposed a new membership change and wedged the current view
                                                         #       }}}}
                                                                  
  # continuing A.4.2 Terminating old view and installing new view after wedging.
  def terminate_epoch():                                 # terminate_epoch() {
    output("in terminate_epoch function")                ### ?? Remove
    output("current changes in changes list are: ")      ### ?? Remove
    for change in sst[my_rank].changes:                  ### ?? Remove
      output("change: ", change)                         ### ?? Remove
                                                         #   //calculate next view membership
    --new_view                                           ## yeild to deliver all pending messages
    leader_rank = curr_view.leader_rank
    committed_count = sst[leader_rank].num_committed - (sst[leader_rank].num_installed)
    output("number of committed_count on leader is: ", committed_count)
                                                         #   committed_count = sst[leader_rank].num_committed − sst[leader_rank].num_installed;
    next_view = View(num_nodes)                          ## creating a new view object
    next_view.members = curr_view.members                #   next_view.members = curr_view.members
    for change_index in range(committed_count):          #   for (change_index = 0; change_index < committed_count; change_index++) {
      node_id = sst[my_rank].changes[change_index]       #      node_id = sst[my_rank].changes[change_index]; 
      output("The current change includes node: ", node_id)
                                                         #     // if node already a member, the change is to remove the node;
      if node_id in next_view.members:                   #     if (curr_view.contains(node_id) == true) {
        output("Removing node/member to the system: ", node_id)
        next_view.remove_member(node_id)                 #       new_view.members.remove(node_id); } ## new_view should be next_view
      else:                                              #     else {
        output("Adding new node/member to the system: ", node_id)
        next_view.add_member(node_id)                    #       next_view.members.append(node_id); }} 
    if leader_rank == my_rank:                           #   if (leader_rank == my_rank) {
      leader_ragged_edge_cleanup()                       #     leader_ragged_edge_cleanup();
    elif await(sst[leader_rank].ragged_edge_computed):   #   else { when (sst[leader_rank].ragged_edge_computed == true) { 
      non_leader_ragged_edge_cleanup()                   #     non_leader_ragged_edge_cleanup(); }}
    curr_view = next_view                                #   curr_view = next_view;
    reset_system(curr_view, sst[my_rank], committed_count)
    output ('new view has been installed with members: ', curr_view.members)#   // |= new view installed                                                         
                                                         # }                  

  ## Cleans up and corrects the ragged edge due to failure, for the leader node
  def leader_ragged_edge_cleanup():                      # leader_ragged_edge_cleanup() {
    output("leader_ragged_edge_cleanup ", self)
    if logical_or("ragged_edge_computed"):               #   if (LogicalOr(sst[∗].ragged_edge_computed) == true) {
      rank = min_with_val("ragged_edge_computed", True)  #     Let rank be s.t. sst[rank].ragged_edge_computed is true
                                                         #     // copy min_latest_received from the node that computed the ragged edge
      for n in range(len(G)):                            #     for (n = 0; n < |G|; ++n) {
        write_sst(my_rank, "min_latest_received", sst[rank].min_latest_received[n], n)
                                                         #       sst[my_rank].min_latest_received[n] = sst[rank].min_latest_received[n]; }
      write_sst(my_rank, "ragged_edge_computed", True)   #     sst[my_rank].ragged_edge_computed = true; }
    else:                                                #   else {
      for n in range(len(G)):                            #     for (n = 0; n < |G|; ++n) {
        write_sst(my_rank, "min_latest_received", min_("latest_received_index", n), n)
                                                         #       sst[my_rank].min_latest_received[n] = Min(sst[∗].latest_received_index[n]);
                                                         #       // |= K_me (sst[my_rank].min_latest_received[n] number of messages from n are safe for delivery) }
      write_sst(my_rank, "ragged_edge_computed", True)   #     sst[my_rank].ragged_edge_computed = true;
    deliver_in_order()                                   #   deliver_in_order(); }
                                              
  ## Cleans up and corrects the ragged edge due to failure, for non-leader nodes
  def non_leader_ragged_edge_cleanup():              # non_leader_ragged_edge_cleanup() {
    output("in non_leader_ragged_edge_cleanup function")
    leader_rank = curr_view.leader_rank              #   // copy from the leader
    for n in range(len(G)):                          #   for (n = 0; n < |G|; ++n) {
      write_sst(my_rank, "min_latest_received", sst[leader_rank].min_latest_received[n], n)
                                                     #     sst[my_rank].min_latest_received[n] = sst[leader_rank].min_latest_received[n]; }
    write_sst(my_rank, "ragged_edge_computed", True) #   sst[my_rank].ragged_edge_computed = true;
    deliver_in_order()                               #   deliver_in_order(); }

  ## Once the ragged edge has been cleaned up, the pending messages are delivered
  def deliver_in_order():                                    # deliver_in_order() {
    output("in deliver_in_order function, messages list: ", msgs)
    curr_global_index = sst[my_rank].latest_delivered_index  #   curr_global_index = sst[my_rank].latest_delivered_index;
    max_global_index = get_max_gi()                          #   max_global_index = max over n of (sst[my_rank].min_latest_received[n] ∗ |G| + n);
    for global_index in range(curr_global_index + 1, max_global_index + 1):
                                                             #   for (global_index = curr_global_index + 1; global_index <= max_global_index; ++global_index) {
      sender_index = global_index / len(G)                   #     sender_index = global_index / |G|;
      sender_rank = global_index % len(G)                    #     sender_rank = global_index % |G|;
      if (sender_index <= sst[my_rank].min_latest_received[sender_rank]):
                                                             #     if (sender_index <= sst[my_rank].min_latest_received[sender_rank]) {
        output("delivering the message with global index ", global_index, " and message ", msgs[global_index])
        deliver_upcall(msgs[global_index], global_index)     #       deliver_upcall(msgs[global_index]); }}}

      

  ## Recieves message for an RDMA write from any node and updates the local SST
  def receive(msg = (_, 'rdma_write_sst', row, col, val, index)):  ## added _ to filter out 'data' and 'control'
    ## update the local SST 
    output("Received message:" , ('rdma_write_sst', row, col, val, index))   
    if index is not None:               ## if a non null 'index' is passed, implies updating a list ### ?? duplicate/redundant code, room for simplification
      newval = getattr(sst[row], col)   ## retrieve the current list into newval
      newval[index] = val               ## update the value at index to the val passed
      setattr(sst [row], col, newval)   ## set the newval to the local sst row
    else:                              
      setattr(sst[row], col, val)       ## if its not a list, just set the value to the new val passed

  ## Recieves message for an RDMA write, update the local view
  def receive(msg = ('rdma_write_view', col, val, index)):
    ## update the local view
    output("Received message:", ('rdma_write_view', col, val, index))
    if index is not None:               ## if a non null 'index' is passed, implies updating a list
      newval = getattr(curr_view, col)  ## retrieve the current list into newval
      newval[index] = val               ## update the value at index to the val passed
      setattr(curr_view, col, newval)   ## set the newval to the local view object
    else:                               
      setattr(curr_view, col, val)      ## if its not a list, just set the value to the new val passed

  ## Reset the system after a membership change in the new epoch
  def reset_system(view, old_sst_row, num_changes_installed):
    self.nodes = set(view.members)        ## nodes in the system based on the members of the given view
    self.num_nodes = len(nodes)           ## number of nodes in the system
    self.G = nodes                        ## nodes in the system, used to calculate global index
    self.sent_num = -1                    ## reset the sent_num in the new epoch
    self.msgs = {}                        ## reset the messages dictionary
    self.sst = [SSTRow(num_nodes, window_size) for _ in range(num_nodes)] ## reinitialize sst 
    self.my_rank = view.members.index(self) ## set my_rank based on the index in the new_view
    write_sst(my_rank, 'changes', old_sst_row.changes[num_changes_installed:])
    write_sst(my_rank, 'num_installed', old_sst_row.num_installed + num_changes_installed)
    write_sst(my_rank, 'num_changes', old_sst_row.num_changes)
    write_sst(my_rank, 'num_committed', old_sst_row.num_committed)
    write_sst(my_rank, 'num_acked', old_sst_row.num_acked)

class RequestMetrics:
  """Metric class to hold all relevant metrics for a particular request"""

  def __init__(self, cpu_start_time, run_start_time):
    self.success = False                  ## maintains the success status of the request
    self.cpu_start_time = cpu_start_time  ## cpu clock time at the time of sending request
    self.cpu_end_time = 0.0               ## cpu clock time at the time of receiving the response for the request 
    self.run_start_time = run_start_time  ## system time at the time of sending request
    self.run_end_time = 0.0               ## system time at the time of receiving the response for the request 

## The rest is for simulation and testing 
class Sim(process):
  """Sim process simulates a failure scenario"""
  def setup(nodes, num_requests, test_failure, msg_size):
    self.metrics = {}                     ## stores the metrics per request id
    self.n_nodes = len(nodes)
    self.received_count = 0

  def run():
    output("Nodes:", nodes)
    failure_injected = False

    for tid in range(num_requests):
      metrics[tid] = RequestMetrics(time.clock(), time.time())
      n = random.sample(nodes,1)
      output("sent request: ", tid, " to node: ", n)
      send(('request', (tid,os.urandom(msg_size))), to=n)

      if test_failure and not failure_injected and tid == num_requests/2:  ## failing last node
        send(('failure', len(nodes)-1), to= list(nodes)[0])  ## send index of the last node as a failed node to all nodes
        ### ?? Delete this
        send(('failure', len(nodes)-2), to= list(nodes)[0])
        output("Sent failure message to first node")
        failure_injected = True
    output('done sending requests to the derecho system')

    while True:
      if await (len(setof(tid, received(('response', tid)))) == num_requests):
        output("---- ---- all responded")
        send(('metrics', list(metrics.values())), to= parent())
        break
      elif timeout(5):
        output("---- ---- timeout!!!")
        for tid in range(num_requests):
          if not metrics[tid].success:
            n = random.sample(nodes,1)
            output('sending message ', tid, ' to node: ', n)
            send(('request', (tid,os.urandom(msg_size))), to=n)
    send('done', to=parent())
   
  def receive(msg= ('response', tid), from_= p):
    if not metrics[tid].success:
      cur_metric = metrics[tid]
      cur_metric.cpu_end_time = time.clock()
      cur_metric.run_end_time = time.time()
      cur_metric.success = True
      received_count += 1
      output("---- Received tid = ", tid, " from ", p, " with received_count: ", received_count)      ## Print the response received from the system

## take the number of nodes as input parameters during runtime, default
def main():
  # config( channel is fifo, clock is Lamport, visualize is True)
  config(channel is fifo, clock is lamport, visualize is {
        # colors: override message and process colors, defaults to random (to fix random?)
        #         supports any valid CSS color value
        # https://developer.mozilla.org/en-US/docs/Web/CSS/color_value
        # examples: Transparent, Yellow, DarkRed, rgb(255, 255 ,0),
        #           rgba(255, 255, 0, 0.1), hsl(210, 100%, 50%)
        'colors': {
            # processes
            #'Sim': 'aquamarine',
            # messages
            'request': 'purple',
            'response': 'blue',
            'data': 'lime',
            'control': 'red',
#            'rdma_write_sst': 'gray',
            'rdma_write_view': 'gray'

        }
    })
  num_nodes = int(sys.argv[1]) if len(sys.argv) > 1 else 5     ## number of nodes in the system
  num_requests = int(sys.argv[2]) if len(sys.argv) > 2 else 200## number of requests from applications
  nreps = int(sys.argv[3]) if len(sys.argv) > 3 else 1         ## number of repetitions to calculate the metrics
  msg_size = int(sys.argv[4]) if len(sys.argv) > 4 else 1      ## size of the message sent to the derecho system
  window_size = 500                                             ## window_size, should be small
  test_failure = True                                          ## set to True to test a failure scenario

  throughputs = []
  while nreps > 0:
    t1 = time.time()
    nodes = new(Node, num= num_nodes)     ## create Node processes
    sim = new(Sim, (nodes, num_requests, test_failure, msg_size))  ## create and set up crash and client simulator process
    for (rank, node) in enumerate(nodes): ## setup Node processes
      setup(node, (nodes, rank, window_size, sim))
    start(nodes)                          ## start Node processes
    start(sim)                            ## start the sim process
    await(received(('done'), from_ =sim)) ## wait to receive 'done' from the sim process
    throughputs.append((time.time() - t1)/num_requests)
    print("--------- time:", time.time() - t1)
    end(nodes)
    end(sim)
    nreps -=1

  cpu_times = []
  run_times = []
  failures = 0
  total_runs = 0
  for metrics in listof(metric, received(("metrics", metric))):
    for metric in metrics:
      total_runs += 1
      if metric.success:
        cpu_times.append(metric.cpu_end_time - metric.cpu_start_time)
        run_times.append(metric.run_end_time - metric.run_start_time)
      else:
        failures += 1

  output("AVG cpu_times", round(sum(cpu_times)/len(cpu_times), 5))
  output("AVG run_times", round(sum(run_times)/len(run_times), 5))
  output("AVG throughput", round(sum(throughputs)/len(throughputs), 5))
  output("FAILURES", failures)