from derecho import Node
from derecho import RequestMetrics
import json
import os
import time
import random
from time import perf_counter

class NodeX(process, Node):
  def setup(checker, nodes, my_rank, window_size, state):
    super().setup(nodes, my_rank, window_size, state)

  def send(m, to):
    super().send(('sent', m, to), to=checker)
    super().send(m, to)

  def receive(msg=m, from_=fr):
    super().send(('rcvd', m, fr), to=checker)

  def send_msg(msg):
    super().send_msg(msg)
    send(('send_msg', msg), to=checker)
    
  def execute(index, req, state):
    res = super().execute(index, req, state)
    (c_id, req_id, _) = req
    send(('execute', index, (c_id, req_id)), to=checker)
    return res

  def deliver_upcall(index, msg):
    super().deliver_upcall(index, msg)
    send(('deliver_upcall', index, msg, logical_clock()), to=checker)
    # if(index == 50 and my_rank == 1):
    #   send(('deliver_upcall', index+1, msg, logical_clock()), to=checker)
    #   send(('deliver_upcall', index+1, req_id, logical_clock()), to=checker)

## LB process is used as load balancer
## The process distributes the load in a round-robin manner among all processes.
## It keeps a count of number of pending messages/requests, and ensures the number
## does not exceed the total number of pending/queued requests in the system.
class LB(process):
  def setup(nodes, num_requests, c_timeout, window_size, test_failure, r_id): 
    self.metrics = {}                     ## stores the metrics per request id
    self.n_nodes = len(nodes)             ## number of active nodes in the system, initially equal to the number of initial nodes
    self.msg_size = 1                     ## size of the message sent to the derecho system
    self.received_count = 0               ## number of responses received
    self.num_pending_requests = 0         ## number of requests pending to be served
    self.max_pending_requests = n_nodes * window_size   ## maximum number of concurrent pending requests served by the system
    self.n_failed = 2                     ## number of nodes to be failed

  def run():
    failure_injected = False              ## tracks if the failure has been injected

    while True:
      if await (len(setof(tid, received(('response', tid, res)))) == num_requests):  ## wait until all the responses have been received
        output("---- all responses received ----")
        send(('end', r_id), to=parent())
        send(('metrics', list(metrics.values())), to= parent())                 ## send the metrics of the requests back to the parent
        break
      elif timeout(c_timeout):
        --receive_pending_msgs
        output("---- timeout, resending " + str(num_requests - received_count) + " number of pending requests ----")
        if received_count == 0:         ### Review
          send(('start'), to=parent())
        n = -1
        for tid in range(num_requests):
          await(timeout(round((random.random())/50, 2)))  ### room for intelligence during this computation
          --receive_pending_msgs
          if tid in metrics and metrics[tid].success:               ## check if request has already been sent and corresponding response received
            continue
          # await (num_pending_requests < max_pending_requests)     ## await for the number of pending requests to drop below the maximum request serving capacity of the system

          if tid not in metrics:                                    ## check if the request has not already been sent
            metrics[tid] = RequestMetrics(time.clock(), perf_counter())## add the request id to the metrics for the new request

          # n = (n+1)%n_nodes                               ## in round-robin manner, select the next server number to send the next request to
          n = random.randint(0, len(nodes)-1)
          output('sending request ', tid, ' to node: ', n)
          send(('request', (self, tid, os.urandom(msg_size))), to=nodes[n])## send the request to the derecho system/server in a round-robin manner
          num_pending_requests += 1                                 ## increment the number of pending requests(non served requests) sent to the system
          
          if test_failure and not failure_injected and tid == num_requests/2:   ## external failure detector, sending failure detection for first and the second last node
            send(('failure', 0), to= nodes[1])            ## send index of the first node/leader node as a failed node to the second node
            send(('failure', n_nodes-2), to= nodes[1])    ## send index of the second last node as a failed node to the second node
            output("Failure messages sent to the last node")
            failure_injected = True                       ## set the boolean to true after the failure message has been sent to the derecho server/system

    output("All requests served, sending done to parent")
    send('done', to=parent())                             ## respond with done to the parent

  def receive(msg= ('response', tid, res)):
    if not metrics[tid].success:
      cur_metric = metrics[tid]
      cur_metric.cpu_end_time = time.clock()
      cur_metric.run_end_time = perf_counter()
      cur_metric.success = True
      received_count += 1
      num_pending_requests -= 1
      output("received response for tid: ", tid, " received_count: ", received_count, " num_pending_requests: ", num_pending_requests)

class Checker(process):
  def setup(nodes, clients, fileName, num_requests, test_failures): 
    self.n_nodes = len(nodes)             ## number of active nodes in the system, initially equal to the number of initial nodes
    self.n_failed = 2                     ## number of nodes to be failed
    self.n_duplicate_requests = 0         ## number of duplicate requests, initially 0
    self.n_noops = 0                      ## number of noops, initially 0

  def run():
    ## Await until every node sends a response back for every client request
    await(len(setof((c, tid, n), received(('sent', ('response', tid, _), c), from_=n))) == num_requests * n_nodes)
    output("All responses have been received from the clients")
    ## Await until all the nodes have updated their latest_delivered_index corresponding to the last response
    max_execute_gi = max(setof(idx, received(('execute', idx, _))))
    output("max_id in checker: ", max_execute_gi)
    await(len(setof(n, received(('sent', ('control','rdma_write_sst', n, 'latest_delivered_index', ldi, _, _), _)), ldi>=max_execute_gi)) == n_nodes) ### optimise?
    await(timeout(20))
    output("All nodes responded to the clients")
    write_statistics()

    n_duplicate_requests = duplicate_requests_count()
    n_noops = noops_count()
    f_handle = open(fileName, "a")
    f_handle.write("************************************************\n")
    f_handle.write("uniform_integrity: " + str(uniform_integrity()) + "\n")
    f_handle.write("validity: " + str(validity()) + "\n")
    f_handle.write("agreement: " + str(agreement()) + "\n")
    f_handle.write("log_consistent: " + str(logs_consistency()) + "\n")
    f_handle.write("log_ordered: " + str(logs_ordering()) + "\n")
    f_handle.write("gi_count_tally: " + str(gi_count_tally()) + "\n")
    f_handle.write("ldi_count_tally: " + str(ldi_count_tally()) + "\n")
    f_handle.write("duplicate_requests_count: " + str(n_duplicate_requests) + "\n")
    f_handle.write("noops_count: " + str(n_noops) + "\n")
    f_handle.write("************************************************\n")
    f_handle.close()
    send('done', to=parent())             ## respond with done to the parent

  # If a server executes an update on seq no i, then the server does not 
  # execute the update on any other seq no i' > i.
  def uniform_integrity():
    output("in uniform_integrity()")
    start_time = perf_counter()
    is_uniform_integrity = each(received(('execute', i1, v), from_=n),
                                has= not some(received(('execute', i2, _v), from_=_n), 
                                      has= i2 > i1))
    end_time = perf_counter()
    output("result of is_uniform_integrity: ", is_uniform_integrity)
    output("time elapsed in computing uniform_integrity: ", end_time - start_time)

    ### The following is a little slower than the above
    # start_time = perf_counter()
    # is_uniform_integrity = each(received(('execute', _, v), from_=n),
    #                             has= len(setof(i, received(('execute', i, _v), from_=_n))) == 1)
    # end_time = perf_counter()
    # output("result of is_uniform_integrity2: ", is_uniform_integrity)
    # output("time elapsed in computing uniform_integrity: ", end_time - start_time)

    ### The following is incorrect, because it does not consider all combinations
    # start_time = perf_counter()
    # is_uniform_integrity = not some(received(('execute', i1, v), from_=n)) or \
    #                         not some(received(('execute', i2, _v), from_=_n), 
    #                                   has= i2 > i1)
    # end_time = perf_counter()
    # output("time elapsed in computing uniform_integrity: ", end_time - start_time)
    return is_uniform_integrity

  # If two servers execute the ith update then these updates are identical.
  def agreement():
    output("in agreement()")
    start_time = perf_counter()
    is_agreement = each(i in setof(i, received(('execute', i, _))),
                      has= len(setof(v, received(('execute', _i, v)))) == 1)
    end_time = perf_counter()
    output("time elapsed in computing agreement: ", end_time - start_time)

    ### The following is about an order of magnitude slower than above
    # start_time = perf_counter()
    # is_agreement = each(received(('execute', i, v1)), 
    #                   received(('execute', i, v2)), 
    #                   has= v1==v2)
    # end_time = perf_counter()
    # output("time elapsed in computing agreement: ", end_time - start_time)
    return is_agreement

  # Only an update that was introduced by a client and subsequently 
  # initiated by a server may be executed.
  def validity():
    output("in validity()")
    start_time = perf_counter()
    is_validity = each(received(('execute', i, (c_id, r_id))),
                      has= some(n in nodes, 
                                received(('rcvd', ('request', (_c_id, _r_id, _)), _), from_=_n), 
                                received(('send_msg', (_c_id, _r_id, _)), from_=_n)))
    end_time = perf_counter()
    output("time elapsed in computing validity: ", end_time - start_time)
    return is_validity

  ## The order of messages delivered across nodes must be consistent
  ### will be redundant if deliver_upcall and execute mean the same
  def logs_consistency():
    output("in logs_consistency()")
    start_time = perf_counter()
    is_logs_consistency = each(i in setof(i, received(('deliver_upcall', i, _, _))),
                              has= len(setof(n, received(('deliver_upcall', _i, _, _), from_=n))) == n_nodes and 
                                    len(setof(r, received(('deliver_upcall', _i, r, _)))) == 1)
    end_time = perf_counter()
    output("time elapsed in computing logs_consistency: ", end_time - start_time)
    return is_logs_consistency

  ## If a node executes an i1 update at time t1 then it did not 
  ## execute i2, i2>i1 update at time t2<t1
  def logs_ordering():
    output("in logs_ordering()")
    start_time = perf_counter()
    is_logs_ordering =  each(received(('deliver_upcall', i1, _, t1), from_=n),
                            has= not some(received(('deliver_upcall', i2, _, t2), from_=_n), 
                                          has=i2>i1 and t2<t1))
    end_time = perf_counter()
    output("time elapsed in computing logs_ordering: ", end_time - start_time)
   
    ### The following is atleast four order of magnitude slower than above
    # output("in logs_ordering2()")
    # start_time = perf_counter()
    # is_logs_ordering = not some(received(('deliver_upcall', i1, _, t1), from_=n)) or \
    #                     not some(received(('deliver_upcall', i2, _, t2), from_=_n),
    #                             has= i2>i1 and t2<t1)
    # end_time = perf_counter()
    # output("is_logs_ordering: ", is_logs_ordering)
    # output("time elapsed in computing logs_ordering: ", end_time - start_time)

    return is_logs_ordering

  ## Return the number of duplicate requests ordered
  def duplicate_requests_count():
    output("in duplicate_requests_count()")
    start_time = perf_counter()
    unique_reqs = setof(req, received(('deliver_upcall', _, req, _)))
    count = len(setof(i, received(('deliver_upcall', i, r, _)),
                            r is not None and 
                            r in unique_reqs)) - num_requests
    end_time = perf_counter()
    output("time elapsed in computing duplicate_requests_count: ", end_time - start_time)
    return count

  ## Return the number of no-ops ordered
  def noops_count():
    output("in noops_count()")
    start_time = perf_counter()
    count = len(setof(i, received(('deliver_upcall', i, r, _)), r is None))
    end_time = perf_counter()
    output("time elapsed in computing noops_count: ", end_time - start_time)
    return count

  ## The total number of ordered messages equals the sum of 
  ## the number of requests, number of no-ops and number of duplicate messages, i.e.,
  ## The total number of ordered messages is given by global index.
  def gi_count_tally():             ### Helped uncover issue in get_min_idx_recv()
    output("in gi_count_tally()")
    start_time = perf_counter()
    max_gi = max(setof(r, received(('sent', ('control','rdma_write_sst', _, 'global_index', r, _, _), _))) 
                                or [-1])
    ## check if max gi across nodes is consistent
    if not len(setof(n, received(('sent', ('control','rdma_write_sst', _, 'global_index', _max_gi, _, _), _), from_=n))) == n_nodes:
      output("global_index inconsistent across nodes")
      return False
    end_time = perf_counter()
    output("time elapsed in computing gi_count_tally: ", end_time - start_time)
    return (max_gi+1) == (num_requests + n_duplicate_requests + n_noops)

  ## Maximum latest_delivered_index equals global_index, and denotes,
  ## total number of ordered messages.
  def ldi_count_tally():
    output("in ldi_count_tally()")
    start_time = perf_counter()
    max_ldi = max(setof(r, received(('sent', ('control','rdma_write_sst', _, 'latest_delivered_index', r, _, _), _))) 
                                or [-1])
    ## check if max ldi across nodes is consistent
    if not len(setof(n, received(('sent', ('control','rdma_write_sst', _, 'latest_delivered_index', _max_ldi, _, _), _), from_=n))) == n_nodes:
      output("latest_delivered_index inconsistent across nodes")
      return False
    end_time = perf_counter()
    output("time elapsed in computing ldi_count_tally: ", end_time - start_time)
    return (max_ldi+1) == (num_requests + n_duplicate_requests + n_noops)

  ## write statistics to the file
  def write_statistics():
    f_handle = open(fileName, "a")                        ## open file to write statistics of the run
    output("Writing statistics to the file")
    log_data("received_num", f_handle)
    log_data("latest_received_index", f_handle)
    log_data("global_index", f_handle)
    log_data("latest_delivered_index", f_handle)
    log_data("num_installed", f_handle)
    log_data("num_committed", f_handle)
    output("Done writing statistics to the file")
    f_handle.close()                                      ## close the file

  def log_data(data_type, f_handle):
    f_handle.write("************************************************\n")
    f_handle.write("*********** " + data_type +" ***********\n")
    for n in range(n_nodes):
      val = max(setof(r, received(('sent', ('control','rdma_write_sst', _n, _data_type, r, _, _), _))) or [0])
      to_write = "node: " + str(n) + " => " + str(val) + "\n"
      f_handle.write(to_write)

  ## Distributes load in the face of failures
  def receive(msg= ('sent', ('control','rdma_write_sst', _, 'num_installed', n_i, _, _), _)):
    if len(setof(n, received(('sent', ('control','rdma_write_sst', _, 'num_installed', _n_failed, _, _), _), from_=n))) == n_nodes-2:
                                        ## if the message with number of installed changes equals number of nodes failed is received 
                                        ##  from all the nodes in the new membership
      nodes.remove(nodes[n_nodes-2])    ## remove the second last node from the nodes list as the node has failed 
      nodes.remove(nodes[0])            ## remove the first node from the nodes list as the node has failed
      n_nodes -= 2                      ## update the number of nodes in the system
      output("nodes have been removed, system has undergone membership change with new nodes as: ", nodes)
      --receive_pending_msgs            ## deliver pending messages received

def main():
  config(channel is fifo, clock is lamport, handling = 'all')
  system_config = read_config("system_config.txt")

  for conf in system_config:    
    num_nodes = conf['num_nodes']       ## number of nodes in the system
    num_clients = conf['num_clients']
    n_req_per_client = conf['num_requests'] ## number of requests to be served
    num_requests = n_req_per_client * num_clients
    window_size = conf['window_size']   ## window_size, number of concurrent pending request per node
    c_timeout = conf['c_timeout']       ## c_timeout, client's timeout before retry of requests
    n_reps = conf['n_reps']             ## number of repetitions to calculate the metrics
    test_failure = conf['test_failure'] ## boolean, if a failure needs to be injected in the system

    c_reps = 0                          ## current count of the rep
    program_run_times = []              ## array of throughput
    program_cpu_times = []
    fileName = os.path.join("statistics", time.strftime("%Y%m%d-%H%M%S") + "_" + str(num_requests) + "_" + 
                            str(num_nodes) + "_" + str(window_size) + "_" + str(test_failure)[0] + ".txt")
    f = open(fileName, "w")             ## create a new file
    f.write("############################################\n" +
            "num_requests: " + str(num_requests) + "\n" + "num_nodes: " + str(num_nodes) + "\n" +           
            "num_clients: " + str(num_clients) + "\n" +"window_size: " + str(window_size) + "\n" + 
            "c_timeout: " + str(c_timeout) + "\n" + "test_failure: " + str(test_failure) + "\n" + 
            "n_reps: " + str(n_reps) + "\n" +
            "############################################\n")
    f.close()                           ## close the file after creation

    while c_reps < n_reps:
      nodes = new(NodeX, num=num_nodes)         ## create Node processes
      clients = new(LB, num=num_clients)
      checker = new (Checker)
      state = []
      for (rank, node) in enumerate(nodes):     ## setup Node processes
        setup(node, (checker, nodes, rank, window_size, state))
      setup(clients, (list(nodes), n_req_per_client, c_timeout, window_size, test_failure, c_reps))
      setup(checker, (list(nodes), list(clients), fileName, num_requests, test_failure))
      start(checker)                            ## start Checker processes
      start(clients)                            ## start Client processes
      start(nodes)                              ## start Node processes
      await(received(('start')))
      t1 = os.times()
      await(each(c in clients, has=some(received(('end', rep_id), from_=c), has=rep_id==c_reps)))
      t2 = os.times()
      cpu_time =  (t2[0] + t2[1] + t2[2] + t2[3]) - (t1[0] + t1[1] + t1[2] + t1[3])
      el_time = t2[4] - t1[4]
      output("el_time: ", el_time)
      output("cpu_time: ", cpu_time)
      await(received(('done'), from_ =checker)) ## wait to receive 'done' from the checker/sim process
      program_run_times.append(el_time)
      program_cpu_times.append(cpu_time)
      end(nodes)
      end(clients)
      end(checker)
      c_reps +=1

    cpu_times = []
    run_times = []
    failures = 0
    for metrics in listof(metric, received(("metrics", metric))):
      for metric in metrics:
        if metric.success:
          cpu_times.append(metric.cpu_end_time - metric.cpu_start_time)
          run_times.append(metric.run_end_time - metric.run_start_time)
        else:
          failures += 1

    average_run_time = (round(sum(program_run_times)/len(program_run_times), 5))
    average_cpu_time = (round(sum(program_cpu_times)/len(program_cpu_times), 5))
    average_throughput_inv = (round(average_run_time/num_requests, 5))
    f = open(fileName, "a")
    f.write("AVG run_Time: " + str(average_run_time) + "\n")
    f.write("AVG average_cpu_time: " + str(average_cpu_time) + "\n")
    f.write("AVG latency: " + str(round(sum(run_times)/len(run_times), 5)) + "\n")
    f.write("AVG throughput: " + str(1/average_throughput_inv) + "\n")
    f.write("FAILURES: " + str(failures) + "\n")
    f.close()

## Reads config from the file pointed to by 'filename'
def read_config(filename):
  with open(filename) as f:
    data = f.read()
    return(json.loads(data))  ## return the json formatted string of the config
